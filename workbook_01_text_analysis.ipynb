{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "518ca145",
   "metadata": {},
   "source": [
    "# Text analysis workbook\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9148c08",
   "metadata": {},
   "source": [
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/MNoichl/data-driven-philosophy-GAP2025/blob/main/workbook_01_text_analysis.ipynb) \n",
    "\n",
    "Welcome to google colab! Colab is a cloud-based notebook environment that allows you to write and execute code in the python programming language in the browser. It follows a notebook structure (like jupyter) in which you can write markdown text like this, as well as code in cells that can be executed.\n",
    "\n",
    "Below is one of these cells. You can run it either by clicking the little (▶️) button on the top left of the cell, or by clicking into it and then pressing shift+enter:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ffcd40",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Hello World\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cede5f6",
   "metadata": {},
   "source": [
    "If you want to continue working on this notebook, and make your own changes to the code, we'd reccomend you save your own copy, by clicking the \"File\" menu at the top left, and then \"Save a copy in Drive\". Please do this as it's easy to loose your work otherwise. You can then edit your own copy. You can also download it as an .ipynb file by clicking the \"File\" menu at the top left, \"Download\", and then \"Download .ipynb\". If you want to learn more about the functionalites of colab notebooks, we reccommend looking at this [basic colab features-notebook.](https://colab.research.google.com/notebooks/basic_features_overview.ipynb)\n",
    "\n",
    "** Important:** Below we are going to run a LLM. This needs a more powerful computer, called an LLM. You can select this by doing Runtime -> Change Runtime Type -> T4 GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7039a9d1",
   "metadata": {},
   "source": [
    "# Part 1: Set-up\n",
    "At the beginning of this notebook, we need to set up all of the libraries/packages (reusable python-programs other people have written) that we are going to use during this session. For this we use a common python-package manager called 'pip'. Pip takes care of downloading the right versions, and installing them on our computer, which in this case is a server that's standing in a google-data-center, maybe in Belgium or Iowa. These installs will take a minute or two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddebf05",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "!pip install pandas\n",
    "!pip install pyalex\n",
    "!pip install umap-learn\n",
    "!pip install datamapplot\n",
    "!pip install upgrade sentence-transformers \n",
    "!pip install seaborn\n",
    "!pip install genieclust\n",
    "!pip install litellm\n",
    "!pip install opinionated\n",
    "!pip install keybert\n",
    "!pip install keyphrase-vectorizers\n",
    "!pip install -q --upgrade \"transformers>=4.45.2\" \"sentence-transformers>=3.0.1\"\n",
    "!pip install --upgrade 'nltk==3.8.1' \n",
    "\n",
    "!python3 -m spacy download en_core_web_sm # for English NER\n",
    "\n",
    "\n",
    "\n",
    "#%pip install -q --upgrade \"transformers==4.44.2\" \"sentence-transformers==2.7.0\" \"accelerate>=0.33\" \"keybert>=0.8.5\" \"torch>=2.2,<2.5\"\n",
    "\n",
    "\n",
    "\n",
    "# Check if utils directory exists, if not download from GitHub\n",
    "import os\n",
    "if not os.path.exists('utils'):\n",
    "    !wget -q https://raw.githubusercontent.com/MNoichl/data-driven-philosophy-GAP2025/main/utils/openalex_utils.py -P utils/\n",
    "    !wget -q https://raw.githubusercontent.com/MNoichl/data-driven-philosophy-GAP2025/main/utils/streamgraph.py -P utils/\n",
    "  \n",
    "  \n",
    "  \n",
    "  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1344e34a",
   "metadata": {},
   "source": [
    "# Part 2: Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746954f0",
   "metadata": {},
   "source": [
    "After setting up the packages, we need to import them. This makes the code in the packages available for us to use later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cab7b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable autoreload for development\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "\n",
    "# Core data science libraries\n",
    "import pandas as pd  # Data manipulation and analysis\n",
    "import numpy as np   # Numerical computing and arrays\n",
    "import os           # Operating system interface\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt  # Basic plotting functionality\n",
    "import seaborn as sns           # Statistical data visualization\n",
    "from matplotlib.colors import rgb2hex\n",
    "\n",
    "# Academic data access\n",
    "import pyalex  # Interface to OpenAlex academic database\n",
    "\n",
    "# Dimensionality reduction and clustering\n",
    "import umap         # Uniform Manifold Approximation and Projection\n",
    "import datamapplot  # Interactive visualization for high-dimensional data\n",
    "\n",
    "# Natural language processing and AI\n",
    "import sentence_transformers  # Sentence embeddings using transformer models\n",
    "import litellm                # Unified interface for various LLM APIs\n",
    "import spacy\n",
    "\n",
    "\n",
    "# Styling and aesthetics\n",
    "import opinionated  # Opinionated matplotlib styling\n",
    "plt.style.use('opinionated_rc')\n",
    "import colormaps as colormaps  # Extended colormap collection - https://pratiman-91.github.io/colormaps/\n",
    "\n",
    "\n",
    "\n",
    "from utils.openalex_utils import openalex_url_to_pyalex_query, process_records_to_df, get_records_from_dois, openalex_url_to_filename, download_openalex_records"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0549c12e",
   "metadata": {},
   "source": [
    "# Part 3: Getting data\n",
    "\n",
    "In this notebook we are interested in analysing textual data. For this we have prepared a dataset of texts from the *Stanford Encyclopedia of Philosophy*. Alternatively, we have set up code that allows you to quickly scrape your own dataset of abstracts from the *OpenAlex*-database. Finally, if you have your owns data-source to work with, we can show you how to hook it up to this notebook below. **Important:** You will only want to run one of the three sub-sections below. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453be5f2",
   "metadata": {},
   "source": [
    "# Part 3.1 Stanford-Encyclopedia dataset\n",
    "\n",
    "The code below downloads a zip-file of SEP-articles, and loads the texts into memory as a list, text_data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2945543",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "\n",
    "# Download and extract if needed\n",
    "stanford_zip = Path(\"stanford-encyclopedia.zip\")\n",
    "stanford_folder = Path(\"stanford-encyclopedia\")\n",
    "if not stanford_zip.exists():\n",
    "    zip_data = requests.get(\"https://github.com/MNoichl/data-driven-philosophy-GAP2025/raw/refs/heads/main/files/stanford-encyclopedia.zip\").content\n",
    "    with zipfile.ZipFile(zipfile.io.BytesIO(zip_data)) as zip_ref:\n",
    "        zip_ref.extractall(\".\")\n",
    "\n",
    "# Load data\n",
    "text_data = [open(f, encoding='utf-8').read() for f in stanford_folder.glob(\"*.md\")]\n",
    "title_data = [f.stem for f in stanford_folder.glob(\"*.md\")]\n",
    "# Extract year data from copyright strings in the text files\n",
    "import re\n",
    "\n",
    "year_data = []\n",
    "for text in text_data:\n",
    "    # Look for \"Copyright © YYYY\" pattern at the beginning of the text\n",
    "    match = re.search(r'Copyright © (\\d{4})', text)  \n",
    "    if match:\n",
    "        year_data.append(int(match.group(1)))\n",
    "    else:\n",
    "        year_data.append(None)  # If no copyright year found\n",
    "\n",
    "used_dataset = \"Stanford Encyclopedia\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3186423a",
   "metadata": {},
   "source": [
    "## Part 3.2 OpenAlex-datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c34791",
   "metadata": {},
   "source": [
    "We have written a function that takes in an arbitrary url to a OpenAlex search query, and downloads the abstracts associated with it. To use it, head over to [https://openalex.org](https://openalex.org), search for something you are interested in, and copy the web-address of your search address. Then replace the url behind ` openalex_url = `  with the new one. Make sure to keep the quotation marks around it. That tells python that this is a string of text, and not executable python code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e54e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "openalex_url = 'https://openalex.org/works?page=1&filter=primary_location.source.id:s255146,publication_year:2005-2025'\n",
    "\n",
    "dataset_df = download_openalex_records(openalex_url,\n",
    "                                       reduce_sample=True, \n",
    "                                       sample_reduction_method=\"n random samples\", \n",
    "                                       sample_size=1000, \n",
    "                                       seed_value=\"42\")\n",
    "\n",
    "\n",
    "dataset_df['text'] = dataset_df['title'] + dataset_df['abstract'] \n",
    "# We filter for works that have an abstract:\n",
    "dataset_df = dataset_df[dataset_df['text'].str.len() > 10]\n",
    "\n",
    "text_data = list(dataset_df['text'])\n",
    "year_data = dataset_df['publication_year']\n",
    "title_data = dataset_df['title']\n",
    "\n",
    "used_dataset = \"OpenAlex-query\"\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1055fb16",
   "metadata": {},
   "source": [
    "# 3.3 Bring your own data\n",
    "\n",
    "Below you can hook up your own dataset. The remainder of the notebook expects `text_data`and  `title_data`to be lists of strings, and  `year_data`to be a list of numbers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b38033",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = \"...\"\n",
    "title_data = \"...\"\n",
    "year_data = \"...\"\n",
    "\n",
    "used_dataset = \"NAME YOUR DATASET HERE\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2c7e7e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "99d5f9e6",
   "metadata": {},
   "source": [
    "# Part 4: Named Entity Recognition (NER)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2f99c7",
   "metadata": {},
   "source": [
    "Named entity recognition is a branch of natural language processing (NLP). Within an NLP pipeline, it is a relatively late step:\n",
    "\n",
    "<center><img src='./files/NER_NLP_pipeline.png' width=700></center>\n",
    "\n",
    "This is because it needs to receive a tokenized text and, in some languages, it needs to understand a word's part-of-speech (POS) to perform well. To determine whether a particular part of a text is a name, our model relies on a tokenization (splitting) of the text and its tagging for a particular part of speech (noun, verb, etc.). \n",
    "\n",
    "*(Graphic from William Mattingly, CC-BY, 2022 Text Analysis Pedagogy Institute)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291a09ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8ebe60",
   "metadata": {},
   "source": [
    "Tokenization distinguishes full stops that end sentences from those that occur in abbreviations and initials (like G. E. Moore) - simply splitting by `.` would not give the right result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6050ff55",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"G.E. Moore is known as a good writer. Perhaps he is also good at programming?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d685af3",
   "metadata": {},
   "source": [
    "We can run the basic spacy pipeline on our text object like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1697647",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba698a43",
   "metadata": {},
   "source": [
    "The resulting doc object is structured. We can inspect its sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf097df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(doc.sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86f70cb",
   "metadata": {},
   "source": [
    "The spacy nlp pipeline also extracts a list of entities. Each entity has a text attribube (from the text source) and a label attribute, which we can inspect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66105bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "[(ent.text, ent.label_) for ent in doc.ents]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3e1a0b",
   "metadata": {},
   "source": [
    "This processing of the raw text into a doc object follows a number of steps that we can inspect (and adapt). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8756c3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4860bfa6",
   "metadata": {},
   "source": [
    "## Looking at the Stanford Encyclopedia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4410420c",
   "metadata": {},
   "source": [
    "Instead of looking at a toy sentence, we can process articles from the Stanford Encyclopedia. Here for a single (arbitrary) article:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25604cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text_data[42])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e18bf2",
   "metadata": {},
   "source": [
    "The NER will now try to detect (and distinguish) persons, organizations, etc. If we only look at the entities tagged as persons:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fdb020",
   "metadata": {},
   "outputs": [],
   "source": [
    "[(ent.text, ent.label_) for ent in doc.ents if ent.label_=='PERSON'][0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ebf56ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'We find {len([ent for ent in doc.ents if ent.label_==\"PERSON\"])} person labelled entities')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf79a974",
   "metadata": {},
   "source": [
    "We can summarize this with a Counter object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3b68e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "count = Counter()\n",
    "count.update([ent.text for ent in doc.ents if ent.label_=='PERSON'])\n",
    "count.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb444c9d",
   "metadata": {},
   "source": [
    "And clean some of the labels by removing the year-strings - this is also important to sum over variants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69b90fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_years(text):\n",
    "    \"\"\"\n",
    "    Clean years and punctuation from labels.\n",
    "    \"\"\"\n",
    "    return re.sub(\"\\s?[0-9]|['|.)(]\",'', text)\n",
    "\n",
    "strip_years(\"Kant's new add\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583807bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = Counter()\n",
    "count.update([strip_years(ent.text) for ent in doc.ents if ent.label_=='PERSON'])\n",
    "count.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a7f297",
   "metadata": {},
   "source": [
    "But have all person names been understood correctly? let's look at the Organizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a84e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = Counter()\n",
    "count.update([strip_years(ent.text) for ent in doc.ents if ent.label_=='ORG'])\n",
    "count.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c7f2ef",
   "metadata": {},
   "source": [
    "A lot of these organization tags are actually referring to people - unsurprising, since many organizations are named after people. So we should expect some errors when trying to distinguish these automatically. \n",
    "\n",
    "In our specific case, we have background knowledge - we expect people to be mentioned much more than companies or organizations. And we know that many of these names are philosophers that should be tagged as persons. To take this into account, we can add an extra step in the NER pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d3abb2",
   "metadata": {},
   "source": [
    "## Adding an entity ruler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fafcfd74",
   "metadata": {},
   "source": [
    "We can add explicit rules for detecting entities with an 'entity ruler'. This acts as an additional step in the spaCy pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2885aeba",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['Stoljar', 'Benson', 'Kymlicka'] # These can be many, feel free to add your own...\n",
    "patterns = [{\"label\":\"PERSON\", \"pattern\":name} for name in names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dced6db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aeff81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp2 = spacy.load(\"en_core_web_sm\", exclude=[\"tagger\",\"parser\",\"lemmatizer\",\"attribute_ruler\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e730294c",
   "metadata": {},
   "outputs": [],
   "source": [
    "entruler = spacy.blank('en') # Create a blank English model\n",
    "nlp_rules = nlp2.add_pipe(\"entity_ruler\", before='ner') # specify that the EntityRuler comes before built-in NER\n",
    "nlp_rules.add_patterns(patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbb04cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp2.pipe_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91d6ff3",
   "metadata": {},
   "source": [
    "The entity ruler now tags the patterns we have defined as persons, before we run the standard NER component. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6363284",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2 = nlp2(text_data[42])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e558ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "count2 = Counter()\n",
    "count2.update([strip_years(ent.text) for ent in doc2.ents if ent.label_=='PERSON'])\n",
    "count2.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ea27b8",
   "metadata": {},
   "source": [
    "Compare again with the top names without our entity ruler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1289c1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text_data[42])\n",
    "count = Counter()\n",
    "count.update([strip_years(ent.text) for ent in doc.ents if ent.label_=='PERSON'])\n",
    "count.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33f2769",
   "metadata": {},
   "source": [
    "Now the explicitly mentioned names are taken into account. Of course, in the raw text there are many more phrases and words that we might want to include as mentions of e.g. Kant:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb90ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Counter()\n",
    "c.update([strip_years(word) for word in text_data[42].split(' ') if re.match('Kant', word)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96491d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46830351",
   "metadata": {},
   "source": [
    "Is 'Kant' mentioned when Korsgaard is described as a Kantian? Even simple counting needs to answer some conceptual questions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00aef47f",
   "metadata": {},
   "source": [
    "## Running on the full corpus\n",
    "(not efficient, can take a while...) Here we are only taking the first 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267ec8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# Sample n random documents if desired\n",
    "import random\n",
    "\n",
    "sampled_texts = random.sample(text_data, 100)\n",
    "\n",
    "\n",
    "docs = list(\n",
    "    tqdm(\n",
    "        nlp2.pipe(sampled_texts, batch_size=1, n_process=max(os.cpu_count()-1, 1)),\n",
    "        total=len(sampled_texts),\n",
    "        unit=\"doc\",\n",
    "        mininterval=0.2\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e34197",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpcount = Counter()\n",
    "for doc in docs:\n",
    "    corpcount.update([strip_years(ent.text) for ent in doc.ents if ent.label_=='PERSON'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18fb0444",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(corpcount.most_common()[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5328c8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get top 20 most common names\n",
    "top_20 = corpcount.most_common(20)\n",
    "names = [item[0] for item in top_20]\n",
    "counts = [item[1] for item in top_20]\n",
    "\n",
    "# Create horizontal bar chart\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.barh(names, counts, color='#980f0e')\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Names')\n",
    "plt.title('Top 20 Most Mentioned Philosophers/Thinkers')\n",
    "plt.gca().invert_yaxis()  # To show highest count at top\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1facacc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2379379",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a2a5b837",
   "metadata": {},
   "source": [
    "# Part 5: Using a language model \n",
    "We are now going to use a text-embedding model (a relatively small large language model) to transform the texts into a format which is easier to analyze mathematically.  The most famous of these models is called BERT (Bidirectional encoder representations from transformers), but there are many models you can choose from for this purpose. Look around e. g. here: [https://www.sbert.net/docs/sentence_transformer/pretrained_models.html ](https://www.sbert.net/docs/sentence_transformer/pretrained_models.html )\n",
    "\n",
    "While they are trained on different datasets and can have a variety of different architectures, their main principles are very similar: Sub-word-particles, called tokens, are associated with long strings of numbers, called embeddings. These embeddings learned during the training-process. When presented with new texts, as we are going to do below, the model cuts the text up into the tokens, and selects the associated embeddings. The embeddings then undergo a process called 'attention' in which the individual representations interact with, and change each-other. In this process, the model adapts the individual embeddings to their context, and changes e.g. the embeddings associated with 'bank', depending on whether the word is close to 'river', as opposed to 'deposit'. Finally, the embeddings of all the tokens in the text get summarized, e.g. by averaging thm. This final embedding, whihc contains information from all the tokens then gets used to represent the whole text.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc184e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model_name = \"all-mpnet-base-v2\" # \"answerdotai/ModernBERT-base\",  \"Qwen/Qwen3-Embedding-0.6B\"\n",
    "model = SentenceTransformer(model_name) #thenlper/gte-small\n",
    "embeddings = model.encode([x[:3000] for x in text_data],\n",
    "                          show_progress_bar=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b4dcb4",
   "metadata": {},
   "source": [
    "This is what the resulting embeddings look like - each row represents one of our texts:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e548778e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf9d456",
   "metadata": {},
   "source": [
    "# Part 6: applying dimensionality reduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024105c3",
   "metadata": {},
   "source": [
    "The attractive thing about text embeddings is that we can calculate similarities between them, which will, due the way that the embedding model is trained, reflect the how similar the topics of the texts are. One way of analyzing these similarities is by using UMAP (Uniform Manifold approximation and Projection). What UMAP effectively does is compute a nearest-neighbour graph, in which each text is linked to the other texts that are most similar to it, reweigh the graph so that high density areas don't dominate, and then layout the resulting network in a lower dimensional space using a force based simulation, in which linked nodes are pulled together, while unconnected nodes are pushed apart. This reproduces important features of outr dataset, whoch originally where encoded in several hundred dimensions in two. For an intuitive explanation to umap, see: [https://pair-code.github.io/understanding-umap/](https://pair-code.github.io/understanding-umap/). Below we produce a layout in two dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2424ebaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "\n",
    "\n",
    "reducer = umap.UMAP(n_components=2, random_state=42,metric='cosine')\n",
    "umap_embeddings = reducer.fit_transform(embeddings)\n",
    "\n",
    "print(umap_embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c4a436",
   "metadata": {},
   "source": [
    "We can also look at the embeddings as a scatter-plot. Each data-point is one of our texts. Note that while the below looks like a scatter plot, the x- and y-axes hold no information. The plot is rotation-invariant, and only the distances between the datapoints are relevant. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a03f5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(umap_embeddings[:, 0], umap_embeddings[:, 1],alpha=0.1, c='black')\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebf0ac6",
   "metadata": {},
   "source": [
    "# Part 5: Clustering \n",
    "\n",
    "To get a better idea of what the the different areas of the plot mean, we can employ clustering. Because the 2-d umap looses a lot of information (768 Dimensions just don't fit that well into 2), and most clustering-algorithms struggle with high dimensional data due to the so-caled *curse of dimensionality*, we run another UMAP-reduction to 30 dimensions, on which we cluster. Here we employ genieclust, a flexible version of agglomerative clustering([https://genieclust.gagolewski.com/weave/basics.html](https://genieclust.gagolewski.com/weave/basics.html)), that allows us to tune how imbalanced we allow cluster sizes to be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f342a64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import genieclust\n",
    "\n",
    "reducer = umap.UMAP(n_components=30, random_state=42,metric='cosine')\n",
    "umap_embeddings_high_dim = reducer.fit_transform(embeddings)\n",
    "\n",
    "g = genieclust.Genie(n_clusters=15, gini_threshold=0.3)\n",
    "cluster_labels = g.fit_predict(umap_embeddings_high_dim)\n",
    "print(cluster_labels[0:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883facf1",
   "metadata": {},
   "source": [
    "We now create a dictionary of colors, which we can use in all the later plots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b50379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique cluster labels\n",
    "unique_clusters = np.unique(cluster_labels)\n",
    "n_clusters = len(unique_clusters)\n",
    "\n",
    "# Get colors from antique colormap, repeating if necessary\n",
    "antique_colors = colormaps.antique(np.linspace(0, 1, colormaps.antique.N))\n",
    "repeated_colors = np.tile(antique_colors, (n_clusters // colormaps.antique.N + 1, 1))[:n_clusters]\n",
    "\n",
    "custom_color_map = dict(\n",
    "    zip(\n",
    "        unique_clusters,\n",
    "        map(rgb2hex, repeated_colors)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d38b3d",
   "metadata": {},
   "source": [
    "And then we plot the umap from above with the clusters overlayed on top:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255c5203",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x=umap_embeddings[:, 0], \n",
    "            y=umap_embeddings[:, 1],\n",
    "            c=[custom_color_map[x] for x in cluster_labels],\n",
    "            alpha=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23adfb4d",
   "metadata": {},
   "source": [
    "# Part 6: Labeling clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4100ed9",
   "metadata": {},
   "source": [
    "We now want to find out what our clusters actually contain. One quick, and a little drity way to do this is to simpy show examples from each cluster to an LLM, and have it come up with a name for it. As we are using a strong LLM here via an API, we need to add an API-key in place of the \"...\" below. API keys are secrets that are directly connected to our credit-cards, so we have to treat them carefully! We are going to send the key for this workshop out via E-Mail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8ac189",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = \"...\"\n",
    "\n",
    "\n",
    "if API_KEY == \"...\": # This code below is only for local development. You can ignore it!\n",
    "    with open('API_KEYS.txt', 'r') as file:\n",
    "        for line in file:\n",
    "            if line.startswith('OPENAI:'):\n",
    "                API_KEY = line.split(':', 1)[1].strip()\n",
    "                break\n",
    "        else:\n",
    "            raise ValueError(\"OPENAI API key not found in API_KEYS.txt\")\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c7aede",
   "metadata": {},
   "source": [
    "Now we can  create our labels. The `label_cluster`function takes in a list of text from each cluster, takes a random sample, and, using the prompt, asks the LLM to name them. Feel free to play with the prompt to steer the labels into the right direction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bde17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from litellm import completion\n",
    "import pandas as pd\n",
    "import random\n",
    "import json\n",
    "\n",
    "# Create a function to label clusters using OpenAI\n",
    "def label_cluster(cluster_texts, cluster_id, n_samples=10):\n",
    "    # Randomly sample representative texts from the cluster (max 10 for efficiency)\n",
    "    sample_size = min(10, len(cluster_texts))\n",
    "    sample_texts = random.sample(cluster_texts, sample_size)\n",
    "    \n",
    "    # Truncate texts to 100 characters\n",
    "    sample_texts = [text[:1000] + \"...\" if len(text) > 1000 else text for text in sample_texts]\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    Below are randomly sampled texts from cluster {cluster_id}. Please analyze these texts and provide:\n",
    "    1. A short descriptive label (2-4 words) for the area of philosophy that the texts are from. Try to pick labels that are commonly used.\n",
    "    2. A brief description of the main theme\n",
    "\n",
    "    Texts:\n",
    "    {chr(10).join([f\"- {text[:200]}...\" if len(text) > 200 else f\"- {text}\" for text in sample_texts])}\n",
    "    \n",
    "    Please respond in JSON format:\n",
    "    {{\n",
    "        \"label\": \"[your label]\",\n",
    "        \"description\": \"[your description]\"\n",
    "    }}\n",
    "    \"\"\"\n",
    "    \n",
    "    messages = [{\"content\": prompt, \"role\": \"user\"}]\n",
    "    response = completion(model=\"openai/gpt-5\", messages=messages, response_format={\"type\": \"json_object\"})\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Group texts by cluster\n",
    "cluster_groups = {}\n",
    "for i, label in enumerate(cluster_labels):\n",
    "    if label not in cluster_groups:\n",
    "        cluster_groups[label] = []\n",
    "    cluster_groups[label].append(text_data[i])\n",
    "\n",
    "# Label each cluster\n",
    "cluster_info = {}\n",
    "for cluster_id, texts in cluster_groups.items():\n",
    "    print(f\"Labeling cluster {cluster_id} ({len(texts)} texts)...\")\n",
    "    label_info = label_cluster(texts, cluster_id, n_samples=5)\n",
    "    cluster_info[cluster_id] = label_info\n",
    "    print(f\"Cluster {cluster_id}: {label_info}\\n\")\n",
    "\n",
    "# Create a summary dataframe\n",
    "cluster_summary = []\n",
    "for cluster_id, info in cluster_info.items():\n",
    "    # Parse the JSON response\n",
    "    try:\n",
    "        parsed_info = json.loads(info)\n",
    "        label = parsed_info.get('label', 'Unknown')\n",
    "        description = parsed_info.get('description', 'No description available')\n",
    "    except (json.JSONDecodeError, TypeError):\n",
    "        # Fallback if JSON parsing fails\n",
    "        label = f\"Cluster {cluster_id}\"\n",
    "        description = str(info)\n",
    "    \n",
    "    cluster_summary.append({\n",
    "        'Cluster_ID': cluster_id,\n",
    "        'Size': len(cluster_groups[cluster_id]),\n",
    "        'Label': label,\n",
    "        'Description': description\n",
    "    })\n",
    "\n",
    "cluster_df = pd.DataFrame(cluster_summary)\n",
    "cluster_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d23108",
   "metadata": {},
   "source": [
    "We also do some data-wrangling, to have one long list of labels for each data-point, and to update our label-dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edcb7503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a labels list from cluster_labels and the labeled cluster descriptions\n",
    "labels_list = []\n",
    "for cluster_id in cluster_labels:\n",
    "    # Find the corresponding label from the cluster_df\n",
    "    cluster_row = cluster_df[cluster_df['Cluster_ID'] == cluster_id]\n",
    "    label = cluster_row.iloc[0]['Label']\n",
    "    labels_list.append(label)\n",
    "\n",
    "# Update the custom_color_map to use cluster labels instead of cluster IDs\n",
    "custom_color_map = {}\n",
    "for cluster_id in unique_clusters:\n",
    "    # Find the corresponding label from the cluster_df\n",
    "    cluster_row = cluster_df[cluster_df['Cluster_ID'] == cluster_id]\n",
    "    label = cluster_row.iloc[0]['Label']\n",
    "    \n",
    "    # Get the color for this cluster_id from the original mapping\n",
    "    color_index = list(unique_clusters).index(cluster_id)\n",
    "    color = rgb2hex(repeated_colors[color_index])\n",
    "    custom_color_map[label] = color\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56abe23b",
   "metadata": {},
   "source": [
    "# Part 7: Visualizing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b42298b",
   "metadata": {},
   "source": [
    "## 7.1 An interactive data-map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab29a29d",
   "metadata": {},
   "source": [
    "To bring everything we've done so far together, we produce an interactive map of our datast. This includes the umap, the clusters and the labels. On hovering over the datapoints we can see their titles. This allows us to explore the structure of our dataset, as well investigate properties, and potential failures of our data and pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cca711f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datamapplot\n",
    "\n",
    "datamapplot.create_interactive_plot(\n",
    "    umap_embeddings,\n",
    "    labels_list,  \n",
    "    hover_text=title_data,  \n",
    "    label_color_map=custom_color_map,\n",
    "    title=used_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5097ab",
   "metadata": {},
   "source": [
    "## 7.2 Historical developments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6913ae64",
   "metadata": {},
   "source": [
    "One common application of topic-modelling/clustering is to investigate the historical development of a corpus. An easy way to accomplish this is a stream-graph, in which the size of each cluster changes as a function of time. We first have to do some data-wrangling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771b9b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for streamgraph visualization\n",
    "# Convert the year_data and clusters into the format expected by plot_streamgraph\n",
    "\n",
    "# Get unique years and sort them\n",
    "years_sorted = sorted(set(year_data))\n",
    "X_stream = np.array(years_sorted)\n",
    "\n",
    "# Get unique clusters and their labels\n",
    "unique_clusters = sorted(set(cluster_labels))\n",
    "cluster_labels_to_plot = []\n",
    "for cluster_id in unique_clusters:\n",
    "    if cluster_id in cluster_df['Cluster_ID'].values:\n",
    "        label = cluster_df[cluster_df['Cluster_ID'] == cluster_id]['Label'].iloc[0]\n",
    "        cluster_labels_to_plot.append(label)\n",
    "    else:\n",
    "        cluster_labels_to_plot.append(f\"Cluster {cluster_id}\")\n",
    "\n",
    "# Create Y matrix: each row is a cluster's values over time\n",
    "Y_stream = []\n",
    "for cluster_id in unique_clusters:\n",
    "    cluster_values = []\n",
    "    for year in years_sorted:\n",
    "        # Count documents in this cluster for this year\n",
    "        cluster_count = sum(1 for i, (doc_year, doc_cluster) in enumerate(zip(year_data, cluster_labels)) \n",
    "                           if doc_year == year and doc_cluster == cluster_id)\n",
    "        cluster_values.append(cluster_count)\n",
    "    Y_stream.append(cluster_values)\n",
    "\n",
    "Y_stream = np.array(Y_stream)\n",
    "\n",
    "print(f\"Streamgraph data prepared:\")\n",
    "print(f\"X (years): {len(X_stream)} points from {X_stream[0]} to {X_stream[-1]}\")\n",
    "print(f\"Y (clusters): {Y_stream.shape[0]} clusters × {Y_stream.shape[1]} time points\")\n",
    "print(f\"Cluster labels: {cluster_labels_to_plot[:3]}...\")  # Show first 3 labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9144c0b",
   "metadata": {},
   "source": [
    "There is no good streamgraph-library in python that we know of, so we made our own and import it here. This version can sort the clusters by size over time, showing which clusters grow and fade proportionally, as well as the growth of the corpus as a whole in absolute numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740df1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.streamgraph import plot_streamgraph\n",
    "# 2) curved edges only, values unchanged\n",
    "plot_streamgraph(X_stream, \n",
    "                 Y_stream, \n",
    "                 labels=cluster_labels_to_plot,\n",
    "                sorted_streams=True,\n",
    "                margin_frac=0.4,\n",
    "                smooth_window=3,\n",
    "                cmap=custom_color_map,\n",
    "                curve_samples=7, label_position='end',label_color='black',\n",
    "    )\n",
    "plt.gca().yaxis.set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ae29b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
