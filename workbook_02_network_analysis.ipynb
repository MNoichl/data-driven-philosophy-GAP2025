{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "518ca145",
   "metadata": {},
   "source": [
    "# Network analysis workbook\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9148c08",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/MNoichl/data-driven-philosophy-GAP2025/blob/main/workbook_02_network_analysis.ipynb) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a85fed",
   "metadata": {},
   "source": [
    "Welcome to google colab! Colab is a cloud-based notebook environment that allows you to write and execute code in the python programming language in the browser. It follows a notebook structure (like jupyter) in which you can write markdown text like this, as well as code in cells that can be executed.\n",
    "\n",
    "Below is one of these cells. You can run it either by clicking the little (▶️) button on the top left of the cell, or by clicking into it and then pressing shift+enter:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19de278c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Hello World\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aacb782b",
   "metadata": {},
   "source": [
    "If you want to continue working on this notebook, and make your own changes to the code, we'd reccomend you save your own copy, by clicking the \"File\" menu at the top left, and then \"Save a copy in Drive\". Please do this as it's easy to loose your work otherwise. You can then edit your own copy. You can also download it as an .ipynb file by clicking the \"File\" menu at the top left, \"Download\", and then \"Download .ipynb\". If you want to learn more about the functionalites of colab notebooks, we reccommend looking at this [basic colab features-notebook.](https://colab.research.google.com/notebooks/basic_features_overview.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7039a9d1",
   "metadata": {},
   "source": [
    "# Part 1: Set-up\n",
    "At the beginning of this notebook, we need to set up all of the libraries/packages (reusable python-programs other people have written) that we are going to use during this session. For this we use a common python-package manager called 'pip'. Pip takes care of downloading the right versions, and installing them on our computer, which in this case is a server that's standing in a google-data-center, maybe in Belgium or Iowa. These installs will take a minute or two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddebf05",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas\n",
    "!pip install pyalex\n",
    "!pip install umap-learn\n",
    "!pip install datamapplot\n",
    "!pip install sentence-transformers\n",
    "!pip install seaborn\n",
    "!pip install genieclust\n",
    "!pip install litellm\n",
    "!pip install opinionated\n",
    "\n",
    "\n",
    "# Check if utils directory exists, if not download from GitHub\n",
    "import os\n",
    "if not os.path.exists('utils'):\n",
    "    !wget -q https://raw.githubusercontent.com/MNoichl/data-driven-philosophy-GAP2025/main/utils/openalex_utils.py -P utils/\n",
    "    !wget -q https://raw.githubusercontent.com/MNoichl/data-driven-philosophy-GAP2025/main/utils/streamgraph.py -P utils/\n",
    "    # Create __init__.py to make it a proper Python package\n",
    "  #  !touch utils/__init__.py\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9c4c0e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1344e34a",
   "metadata": {},
   "source": [
    "# Part 2: Imports\n",
    "After setting up the packages, we need to import them. This makes the code in the packages available for us to use later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cab7b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Our data-handling library:\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Our visualisation libraries:\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# numerical calculations:\n",
    "import numpy as np\n",
    "\n",
    "# Interacting with the operating system:\n",
    "import os\n",
    "\n",
    "# Interacting with the OpenAlex API:\n",
    "import pyalex\n",
    "\n",
    "# Our network analysis library:\n",
    "import networkx as nx\n",
    "\n",
    "\n",
    "\n",
    "# Library for prettier graphics:\n",
    "import opinionated\n",
    "plt.style.use('opinionated_rc')\n",
    "import colormaps as colormaps #https://pratiman-91.github.io/colormaps/\n",
    "\n",
    "\n",
    "# Some utility functions for interacting with the OpenAlex API:\n",
    "from utils.openalex_utils import openalex_url_to_pyalex_query, process_records_to_df, get_records_from_dois, openalex_url_to_filename, download_openalex_records"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0549c12e",
   "metadata": {},
   "source": [
    "# Part 3: Getting data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ebf9a9e",
   "metadata": {},
   "source": [
    "To get our network-data, we are going to query the open-alex database. To make this easy,  we have written a function that takes in an arbitrary url to a OpenAlex search query, and downloads the abstracts associated with it. To use it, head over to [https://openalex.org](https://openalex.org), search for something you are interested in, and copy the web-address of your search address. Then replace the url behind ` openalex_url = `  with the new one. Make sure to keep the quotation marks around it. That tells python that this is a string of text, and not executable python code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e54e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "openalex_url = 'https://openalex.org/works?page=1&filter=primary_location.source.id:s255146,publication_year:2005-2025'\n",
    "\n",
    "dataset_df = download_openalex_records(openalex_url,\n",
    "                                       reduce_sample=True, \n",
    "                                       sample_reduction_method=\"n random samples\", \n",
    "                                       sample_size=5000, \n",
    "                                       seed_value=\"42\")\n",
    "\n",
    "\n",
    "dataset_df['text'] = dataset_df['title'] + dataset_df['abstract'] \n",
    "# We filter for works that have an abstract:\n",
    "dataset_df = dataset_df[dataset_df['text'].str.len() > 10]\n",
    "\n",
    "text_data = list(dataset_df['text'])\n",
    "year_data = dataset_df['publication_year']\n",
    "title_data = dataset_df['title']\n",
    "\n",
    "used_dataset = \"OpenAlex-query\"\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e3fbd1",
   "metadata": {},
   "source": [
    "We can take a look at the data, by displaying the dataframe-object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7de8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(dataset_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6797c3",
   "metadata": {},
   "source": [
    "One column is particularly interesting to us, `referenced_works`, which contains lists with ids for the cited articles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4154fdb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df['referenced_works']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e43a9a",
   "metadata": {},
   "source": [
    "# Part 4: Constructing a network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a591b6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the dataset into an author co-citation network\n",
    "\n",
    "\n",
    "# Create an undirected graph for author co-citation network\n",
    "G = nx.Graph()\n",
    "\n",
    "# First, collect all authors and their papers\n",
    "author_to_papers = {}\n",
    "paper_to_authors = {}\n",
    "\n",
    "# Extract authors from each paper\n",
    "for idx, row in dataset_df.iterrows():\n",
    "    paper_id = row['id']\n",
    "    \n",
    "    # Get authors for this paper\n",
    "    if 'authorships' in row and row['authorships'] is not None:\n",
    "        authors = []\n",
    "        authorships = row['authorships']\n",
    "        \n",
    "        # Handle different formats of authorships\n",
    "        if isinstance(authorships, list):\n",
    "            for authorship in authorships:\n",
    "                if isinstance(authorship, dict) and 'author' in authorship:\n",
    "                    author_info = authorship['author']\n",
    "                    if isinstance(author_info, dict) and 'id' in author_info:\n",
    "                        author_id = author_info['id']\n",
    "                        author_name = author_info.get('display_name', author_id)\n",
    "                        authors.append((author_id, author_name))\n",
    "        \n",
    "        # Store author-paper relationships\n",
    "        if authors:\n",
    "            paper_to_authors[paper_id] = authors\n",
    "            for author_id, author_name in authors:\n",
    "                if author_id not in author_to_papers:\n",
    "                    author_to_papers[author_id] = []\n",
    "                author_to_papers[author_id].append(paper_id)\n",
    "\n",
    "# Add author nodes to the graph\n",
    "for author_id, papers in author_to_papers.items():\n",
    "    # Get author name from the first paper\n",
    "    author_name = author_id\n",
    "    for paper_id in papers:\n",
    "        if paper_id in paper_to_authors:\n",
    "            for aid, aname in paper_to_authors[paper_id]:\n",
    "                if aid == author_id:\n",
    "                    author_name = aname\n",
    "                    break\n",
    "            break\n",
    "    \n",
    "    G.add_node(author_id, \n",
    "               name=author_name,\n",
    "               paper_count=len(papers))\n",
    "\n",
    "# Create co-citation edges\n",
    "cocitation_count = 0\n",
    "for idx, row in dataset_df.iterrows():\n",
    "    citing_paper = row['id']\n",
    "    \n",
    "    # Get the referenced works (citations) for this paper\n",
    "    if 'referenced_works' in row and row['referenced_works'] is not None and len(row['referenced_works']) > 0:\n",
    "        referenced_works = row['referenced_works']\n",
    "        \n",
    "        # Handle different formats of referenced_works\n",
    "        if isinstance(referenced_works, str):\n",
    "            try:\n",
    "                referenced_works = referenced_works.strip('[]').split(',')\n",
    "                referenced_works = [ref.strip().strip(\"'\\\"\") for ref in referenced_works if ref.strip()]\n",
    "            except:\n",
    "                referenced_works = []\n",
    "        elif isinstance(referenced_works, list):\n",
    "            pass\n",
    "        else:\n",
    "            referenced_works = []\n",
    "        \n",
    "        # Find authors of cited papers within our dataset\n",
    "        cited_authors = []\n",
    "        for cited_paper in referenced_works:\n",
    "            if cited_paper in paper_to_authors:\n",
    "                cited_authors.extend([author_id for author_id, _ in paper_to_authors[cited_paper]])\n",
    "        \n",
    "        # Create co-citation edges between authors who are cited together\n",
    "        for i in range(len(cited_authors)):\n",
    "            for j in range(i + 1, len(cited_authors)):\n",
    "                author1, author2 = cited_authors[i], cited_authors[j]\n",
    "                # Avoid self-loops by ensuring authors are different\n",
    "                if author1 != author2 and author1 in G.nodes() and author2 in G.nodes():\n",
    "                    if G.has_edge(author1, author2):\n",
    "                        G[author1][author2]['weight'] += 1\n",
    "                    else:\n",
    "                        G.add_edge(author1, author2, weight=1)\n",
    "                    cocitation_count += 1\n",
    "\n",
    "print(f\"Author co-citation network created with {G.number_of_nodes()} nodes and {G.number_of_edges()} edges\")\n",
    "print(f\"Total co-citation relationships: {cocitation_count}\")\n",
    "\n",
    "# Display basic network statistics\n",
    "print(f\"\\nNetwork Statistics:\")\n",
    "print(f\"- Nodes (authors): {G.number_of_nodes()}\")\n",
    "print(f\"- Edges (co-citations): {G.number_of_edges()}\")\n",
    "print(f\"- Density: {nx.density(G):.4f}\")\n",
    "print(f\"- Is connected: {nx.is_connected(G)}\")\n",
    "\n",
    "if G.number_of_edges() > 0:\n",
    "    print(f\"- Average degree: {sum(dict(G.degree()).values()) / G.number_of_nodes():.2f}\")\n",
    "    \n",
    "    # Show top co-cited author pairs\n",
    "    edge_weights = [(G[u][v]['weight'], u, v) for u, v in G.edges()]\n",
    "    edge_weights.sort(reverse=True)\n",
    "    print(f\"\\nTop 5 co-cited author pairs:\")\n",
    "    for i, (weight, author1, author2) in enumerate(edge_weights[:5]):\n",
    "        name1 = G.nodes[author1].get('name', author1)\n",
    "        name2 = G.nodes[author2].get('name', author2)\n",
    "        print(f\"  {i+1}. {name1} & {name2} (co-cited {weight} times)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afda3a17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1031d4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the largest connected component (for undirected graphs)\n",
    "if nx.is_directed(G):\n",
    "    largest_cc = max(nx.weakly_connected_components(G), key=len)\n",
    "else:\n",
    "    largest_cc = max(nx.connected_components(G), key=len)\n",
    "G_giant_component = G.subgraph(largest_cc).copy()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4bb3d2",
   "metadata": {},
   "source": [
    "# Community detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158d5eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from networkx.algorithms.community import louvain_communities\n",
    "\n",
    "print(\"Detecting communities using Louvain algorithm...\")\n",
    "communities = louvain_communities(G_giant_component, \n",
    "                                weight='weight', \n",
    "                                resolution=1.0, \n",
    "                                seed=42)\n",
    "\n",
    "print(f\"Found {len(communities)} communities\")\n",
    "\n",
    "# Create a dictionary mapping nodes to community IDs\n",
    "node_to_community = {}\n",
    "for i, community in enumerate(communities):\n",
    "    for node in community:\n",
    "        node_to_community[node] = i\n",
    "\n",
    "# Add community information to nodes\n",
    "nx.set_node_attributes(G_giant_component, node_to_community, 'community')\n",
    "\n",
    "# Also add community information to the original graph G\n",
    "nx.set_node_attributes(G, node_to_community, 'community')\n",
    "\n",
    "# Display community statistics\n",
    "community_sizes = [len(community) for community in communities]\n",
    "print(f\"\\nCommunity Statistics:\")\n",
    "print(f\"- Number of communities: {len(communities)}\")\n",
    "print(f\"- Largest community size: {max(community_sizes)}\")\n",
    "print(f\"- Smallest community size: {min(community_sizes)}\")\n",
    "print(f\"- Average community size: {sum(community_sizes) / len(community_sizes):.2f}\")\n",
    "\n",
    "# Show top 5 largest communities\n",
    "sorted_communities = sorted(enumerate(communities), key=lambda x: len(x[1]), reverse=True)\n",
    "print(f\"\\nTop 5 largest communities:\")\n",
    "for i, (comm_id, community) in enumerate(sorted_communities[:5]):\n",
    "    print(f\"  Community {comm_id}: {len(community)} authors\")\n",
    "    # Show a few example authors from each community\n",
    "    sample_authors = list(community)[:3]\n",
    "    author_names = [G_giant_component.nodes[author].get('name', author) for author in sample_authors]\n",
    "    print(f\"    Examples: {', '.join(author_names)}\")\n",
    "    if len(community) > 3:\n",
    "        print(f\"    ... and {len(community) - 3} more\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91614cd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63d4f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ForceAtlas2 layout and visualization using NetworkX\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "node_degrees = dict(G_giant_component.degree())\n",
    "\n",
    "    # Use NetworkX ForceAtlas2 layout\n",
    "print(\"Computing ForceAtlas2 layout...\")\n",
    "positions = nx.forceatlas2_layout(G_giant_component, \n",
    "                                    max_iter=500,  # Number of iterations\n",
    "                                    jitter_tolerance=1.0,  # Tolerance for speed adjustment\n",
    "                                    scaling_ratio=2.,  # Scaling of forces\n",
    "                                    gravity=5,  # Attraction to center\n",
    "                                    distributed_action=False,  # Distribute attraction force\n",
    "                                    node_size = {n: np.log(d+1) * 2 for n, d in G_giant_component.degree()},\n",
    "                                    strong_gravity=False,  # Strong gravitational pull\n",
    "                                    weight='weight',  # Edge weight attribute\n",
    "                                    dissuade_hubs=True,  # Prevent hub clustering\n",
    "                                    linlog=False,  # Use linear attraction\n",
    "                                    seed=42,  # Random seed for reproducibility\n",
    "                                    dim=2)  # 2D layout\n",
    "\n",
    "\n",
    "print('Plotting the network...')\n",
    "# Create the plot\n",
    "plt.figure(figsize=(15, 12))\n",
    "\n",
    "# Draw the network\n",
    "# Scale node sizes by degree\n",
    "\n",
    "node_sizes = [node_degrees[node] * 5 + 5 for node in G_giant_component.nodes()]\n",
    "\n",
    "# Color nodes by community\n",
    "node_colors = [node[1]['community'] for node in G_giant_component.nodes(data=True)]\n",
    "\n",
    "nx.draw_networkx_nodes(G_giant_component, positions, \n",
    "                      node_size=node_sizes, \n",
    "                      node_color=node_colors, \n",
    "                      cmap=plt.cm.Set3,\n",
    "                      alpha=0.7)\n",
    "\n",
    "nx.draw_networkx_edges(G_giant_component, positions, \n",
    "                      edge_color='gray', \n",
    "                      alpha=0.3, \n",
    "                      arrows=True, \n",
    "                      arrowsize=10,\n",
    "                      arrowstyle='-')\n",
    "\n",
    "plt.title(\"Citation Network - ForceAtlas2 Layout\", fontsize=16)\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"ForceAtlas2 layout completed for {G.number_of_nodes()} nodes and {G.number_of_edges()} edges\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf23dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute PageRank on the network\n",
    "print(\"Computing PageRank...\")\n",
    "pagerank_scores = nx.pagerank(G, weight='weight', alpha=0.85, max_iter=100, tol=1e-06)\n",
    "\n",
    "# Get top 10 nodes by PageRank score\n",
    "top_10_pagerank = sorted(pagerank_scores.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "\n",
    "print(\"\\nTop 10 nodes by PageRank score:\")\n",
    "print(\"=\" * 50)\n",
    "for i, (node_id, score) in enumerate(top_10_pagerank, 1):\n",
    "    # Get node name if available\n",
    "    node_name = G.nodes[node_id].get('name', 'Unknown')\n",
    "    print(f\"{i:2d}. {node_name}\")\n",
    "    print(f\"    ID: {node_id}\")\n",
    "    print(f\"    PageRank: {score:.6f}\")\n",
    "    print(f\"    Degree: {G.degree(node_id)}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c54a005",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f0931d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09fc6ce2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1783efe6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83cc6ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
