{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e89c5b3",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "<img align=\"left\" src=\"./files/CC_BY.png\"><br />\n",
    "\n",
    "This notebook was created by Gregor Bös and Maximilian Noichl for the 2025 GAP.12 satellite workshop 'Data-Driven Methods for Philosophy'.\n",
    "\n",
    "It is based on notebooks created by William Mattingly for the 2022 Text Analysis Pedagogy Institute, with support from the [National Endowment for the Humanities](https://neh.gov), [JSTOR Labs](https://labs.jstor.org/), and [University of Arizona Libraries](https://new.library.arizona.edu/), with adaptations by Zhuo Chen under [Creative Commons CC BY License](https://creativecommons.org/licenses/by/4.0/).\n",
    "\n",
    "For questions/comments/improvements, email mail@gregorboes.com, m.noichl@uu.nl, zhuo.chen@ithaka.org or nathan.kelber@ithaka.org.<br />\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f932d1",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# MultiLingual NER\n",
    "\n",
    "This is a notebook on **Named Entity Recognition**. \n",
    "\n",
    "**Chapter I: NER, encoding, and multilingual corpora**\n",
    "**Chapter II: Introduction to spaCy**\n",
    "**Chapter III: Word Embeddings and Entity Rulers**\n",
    "\n",
    "**Use case:** For Learning (Detailed explanation, not focused on immediate research application)\n",
    "\n",
    "**Difficulty:** Intermediate\n",
    "\n",
    "**Knowledge Required:** \n",
    "\n",
    "* Python basics\n",
    "\n",
    "**Knowledge Recommended:**\n",
    "\n",
    "* Natural Language Processing\n",
    "\n",
    "**Data Format**: .txt\n",
    "\n",
    "**Libraries Used**: spaCy___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaac256a-b731-40dc-a2f2-7bbc432d22f9",
   "metadata": {},
   "source": [
    "Motivating NER for Philosophy (corpus-based methods, good for quantifiying historical claims, structuring large amounts of text...)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a44149-e47f-44b9-a885-b4544a4dabc6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "source": [
    "# Introduction to Named Entity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e3c4ab",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Named Entity Recognition\n",
    "**Entities** are words in a text that correspond to a specific type of data. For example, we may find the following types of entities in a text.\n",
    "* numerical, such as cardinal numbers; \n",
    "* temporal, such as dates; \n",
    "* nominal, such as names of people and places;\n",
    "* political, such as geopolitical entities (GPE). \n",
    "\n",
    "Named entity recognition, or NER, is the process by which a system takes an input of a text and outputs the identification of entities.\n",
    "\n",
    "### A simple example\n",
    "Let's use the following sentence as an example.\n",
    "\n",
    "*Martha, a senior, moved to Spain where she will be playing basketball until 05 June 2022 or until she can't play any longer.*\n",
    "\n",
    "First, there is \"Martha\", a person's name. Different NER models may give the label of PERSON or PER to it.\n",
    "\n",
    "Second, there is \"Spain\", a country name. It is a GPE, or Geopolitical Entity.\n",
    "\n",
    "Finally, there is \"05 June 2022\", a date. It is a DATE entity. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33cc2fc",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "In this series, we are going to use the SpaCy library to do NER. Here is a preview of how spaCy identifies entities in a text string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94945221-e457-44b5-81a4-5912a5e5a27b",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install(package):\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "# Install your specific packages\n",
    "packages = [\n",
    "    'beautifulsoup4==4.12.2',\n",
    "    'click==8.1.3',\n",
    "    'gensim==4.3.1',\n",
    "    'ipympl==0.9.3',\n",
    "    'jupyter-ai==2.19.1',\n",
    "    'jupyter-ai-magics==2.19.0',\n",
    "    'jupyterlab-git==0.50.0',\n",
    "    'matplotlib==3.8.4',\n",
    "    'numpy>=1.16',\n",
    "    'nltk==3.9.1',\n",
    "    'openai==1.51.0',\n",
    "    'pandas>=2.0.3',\n",
    "    'pillow==10.3.0',\n",
    "    'pyarrow==14.0.1',\n",
    "    'pyldavis==3.4.1',\n",
    "    'pytesseract==0.3.10',\n",
    "    'regex==2023.6.3',\n",
    "    'requests==2.32.3',\n",
    "    'scikit-learn==1.5.1',\n",
    "    'scipy==1.11.1',\n",
    "    'seaborn==0.12.2',\n",
    "    'spacy==3.5.4',\n",
    "    'urllib3==2.2.2',\n",
    "    'vadersentiment==3.3.2',\n",
    "    'wordcloud==1.9.2',\n",
    "    'zipp==3.19.2'\n",
    "]\n",
    "\n",
    "for package in packages:\n",
    "    install(package)\n",
    "\n",
    "# Additional setup for specific packages that need extra data/models\n",
    "print(\"Setting up additional package data...\")\n",
    "\n",
    "# NLTK data downloads\n",
    "try:\n",
    "    import nltk\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "    nltk.download('vader_lexicon', quiet=True)\n",
    "    nltk.download('wordnet', quiet=True)\n",
    "    nltk.download('omw-1.4', quiet=True)\n",
    "    print(\"✓ NLTK data downloaded\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠ NLTK setup issue: {e}\")\n",
    "\n",
    "print(\"Package installation and setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969a8336-e624-4bf2-939b-e00ffee1886f",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "### for running spacy displacy later\n",
    "from IPython.display import display\n",
    "sys.modules['IPython.core.display'] = sys.modules['IPython.display']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249cbf38",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# download the small English NLP model from spacy\n",
    "!python3 -m spacy download en_core_web_sm # for English NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a985438",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import spacy\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b605c8cd",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load the small English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Create a doc object out of the text string\n",
    "sentence = \"\"\"Martha, a senior, moved to Spain where she will be playing basketball until 05 June 2022 \n",
    "or until she can't play any longer.\"\"\"\n",
    "doc = nlp(sentence)\n",
    "\n",
    "# Get the entities from the doc\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a63c21-198c-4858-96e2-478fb41fb350",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9777476-480d-40a9-aeef-89144fc79499",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Natual Language Processing (NLP) is the process by which a researcher uses a computer system to parse human language and extract important information from texts.\n",
    "\n",
    "How do we extract information from texts? We do it through a series of pipelines that perform some operations on the data at hand.\n",
    "\n",
    "<center><img src='./files/NER_NLP_pipeline.png' width=700></center>\n",
    "\n",
    "Named entity recognition is a branch of natural language processing. From the graph, you may notice that named entity recognition comes later in NLP. This is because it needs to receive a tokenized text and, in some languages, it needs to understand a word's part-of-speech (POS) to perform well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354529f1",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Tokenizer\n",
    "The job of a tokenizer is to break a text into individual tokens. Tokens are items in a text that have some linguistic meaning. They can be words, such as \"Martha\", but they can also be punctuation marks, such as \",\" in the relative clause \", a senior,\". Likewise, \"n't\" in the contraction \"can't\" would also be recognized as a token since \"n't\" in English corresponds to the word \"not\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b137d625",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the tokens in the doc object we created\n",
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23465ce",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### POS tagger\n",
    "A common pipeline after a tokenizer is a POS tagger whose job is to identify the parts-of-speech, or POS, in the text. Let us consider an example sentence:\n",
    "\n",
    "The boy took the ball to the store.\n",
    "\n",
    "The nominative (subject), \"boy\", comes first in the sentence, followed by the verb, \"took\", then followed by the accusative (object), \"ball\", and finally the dative (indirect object), \"store\". The words \"the\" and \"to\" also contain vital information. \"The\" occurs twice and tells the reader that it's not just any ball, it's the ball; likewise, it's not just a store, but the store. The period too tells us something important. This is a statement, not a question. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3638f9d9",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import the visualizer in spaCy\n",
    "from spacy import displacy\n",
    "\n",
    "# Visualize the POS tags and syntactic dependencies using displacy.serve() function\n",
    "displacy.render(doc, style=\"dep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1450f0",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get all pos tags\n",
    "dir(spacy.parts_of_speech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4acdd6a2",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the meaning of a certain tag\n",
    "spacy.explain('PROPN')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f803f59c-d7f8-48ff-af5a-b10aa0b8fbdb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "source": [
    "# Text Encoding\n",
    "\n",
    "Before we tokenize a text, we have to know what encoding it uses. \n",
    "\n",
    "When we say \"plain text\", we are actually being really sloppy. There is no such thing as \"plain text\"! It does not make sense to have a text without knowing what encoding it uses. \n",
    "\n",
    "Have you ever received an email in which you find the text is unintelligible because there are random question marks in it? Did you wonder why?\n",
    "\n",
    "## A little bit history about text encoding\n",
    "\n",
    "### ASCII (American Standard Code for Information Interchange)\n",
    "There was a time when the only characters that mattered were good old unaccented English letters. ASCII is a code which was able to represent every character using a number between 32 and 127 (Codes below 32 were called unprintable and for control characters, e.g. the backspace). For example, the ASCII code of the letter A is 65. As you know, computers use a binary system and therefore the number 65 is actually encoded as a 8-bit number.\n",
    "\n",
    "A $\\rightarrow$ 0100 0001\n",
    "\n",
    "8-bit allows up to $2^{8}=256$ characters and we have only had 128 (with numbers 0-127). That means we can use the numbers 128 to 255 to represent other characters! English, of course, is not the only language that matters. Therefore, people speaking different languages chose to use the numbers 128 to 255 for the characters in their own language. This means that two different characters from two different languages may be represented by the same number in their respective encoding standard. This is no good, because when Americans would send their résumés to Israel they would arrive as rגsumגs. \n",
    "\n",
    "\n",
    "### UTF-8\n",
    "Can't we have a single character set that includes every reasonable writing system on the planet? Yes we can! \n",
    "Here comes the brilliant idea of UTF-8. UTF stands for Unicode Transformation Format. 8 means 8-bit.\n",
    "\n",
    "Every letter in every alphabet is assigned a number written like this: U+0041. It is called a *code point*. The U+ means \"Unicode\" and the numbers are hexadecimal (from 0 to F). In Python, code points are written in the form \\uXXXX, where XXXX is the number in 4-digit hexadecimal form. The English letter A is assigned the number U+0041.\n",
    "\n",
    "UTF-8 was designed for backward compatibility with ASCII: the first 128 characters of Unicode, which correspond one-to-one with ASCII, are encoded using a single byte with the same binary value as ASCII.\n",
    "\n",
    "## ord( ) and chr( )\n",
    "\n",
    "`ord()` and `chr()` are built-in functions in Python.\n",
    "\n",
    "The `ord()` function takes a single Unicode character and returns its integer Unicode code point value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4aec386",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use ord() to convert a Unicode character string to its integer code point value\n",
    "\n",
    "ord(\"ø\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcea73bb",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "The `chr()` function does the opposite. It takes an integer and returns the corresponding Unicode character. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a350edd",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use chr() to convert a number to a character\n",
    "\n",
    "chr(248)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5981b2",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## What does all this mean to Named Entity Recognition?\n",
    "This means that 'plain text' is just a mystery. Computers ultimately only get a sequence of numbers and what characters those numbers translate to depends on the encoding. \n",
    "\n",
    "For those who work with multilingual corpora, especially those who work with texts that were created before the modern day, you will encounter at some point corpora that contain multiple encodings. We can use Python, however, to read a different encoding, standardize it into utf-8, and then continue to open that file as a utf-8 file consistently in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38613ed9",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use Python3 to read a polish text with latin2 encoding\n",
    "with open(path, 'r', encoding='latin2') as f:\n",
    "    pol_data=f.read()\n",
    "print(pol_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d154cfc",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Write the data with the encoding utf8 to another file\n",
    "with open('./data/pol_lat2_to_utf8.txt', 'w', encoding='utf8') as f:\n",
    "    f.write(pol_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faacec54",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Open the new file using utf8\n",
    "with open('./data/pol_lat2_to_utf8.txt', 'r', encoding='utf8') as f:\n",
    "    pol_utf8=f.read()\n",
    "print(pol_utf8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9d7e72",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "When we print out the Polish data from the file encoded in latin2 and from the one encoded in utf8, we get two strings on the screen that look exactly the same. However, when we get the byte strings from the two files, we see that even if two characters look the same to our naked eye, e.g. ń, they are different at the byte level. This means the computer will see them as two different characters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4042561e",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Again, for those who work with multilingual corpora, you will encounter at some point corpora that contain multiple encodings. It is best practice to always convert your data to utf8 before proceeding to tokenization and NER. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521c5006",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "<h2 style=\"color:red; display:inline\">Coding Challenge! &lt; / &gt; </h2>\n",
    "\n",
    "Here is a txt file that is encoded in ISO-8859-15. Can you convert it to a utf-8 file?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfa920d",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the file\n",
    "file = './files/iso_text.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60fd83f",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4b0bbe26-b237-4e04-8d38-46924e346f4a",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "source": [
    "## Problems within UTF-8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305d67dc-4b5d-414f-b2e2-356f3baa7f9a",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Our problems with encodings, unfortunately, do not end with UTF-8. Once we have encoded our texts into UTF-8, we can still have issues with characters that look the same but being encoded differently. This is particularly true with accented characters.\n",
    "\n",
    "Here we have two characters that look exactly the same and are also deemed the same by the computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2cd950-aa3d-4386-9d03-473f536c9b96",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Two characters that look exactly the same are deemed the same\n",
    "\"Ç\" == \"Ç\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07f9a29",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Here, we also have two characters that look exactly the same but this time they are deemed as two different characters by the computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7096345-bb1f-4903-9889-147fef0a6b56",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Two characters that look exactly the same are not deemed the same\n",
    "\"Ç\" == \"Ç\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed26c8ae",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "The two characters are regarded as different by the computer because at the byte level, they are different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34560c53",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Print out the unicode strings for the two characters\n",
    "print(\"\\u00C7\", \"\\u0043\\u0327\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85274a3",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "One of them is seen as a single character, the accented C. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5046a64a-deba-415e-a2fc-3ca9733fbce2",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Latin capital letter C with cedilla\n",
    "accent_c = \"\\u00C7\"\n",
    "print(accent_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac1d46c",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "The other is seen as a compound character, consisting of two characters, one being the Latin letter C and the other being the 'combining cedilla' character. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad27755e-2210-4d2c-b07d-8a3fe2da95fd",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 'Latin capital letter C' and 'combining cedilla' characters together\n",
    "compound_c = \"\\u0043\\u0327\"\n",
    "print(compound_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871f1212",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "The 'combining cedilla' character can be combined with other letters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4098ff",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Another example of compound character\n",
    "\"J\\u0327\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377c3056",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Take a look at the two parts of the compound C\n",
    "\"\\u0043 \\u0327\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6fd59b5",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Because the compound C and the accented C are different at the byte level, they are not considered the same by the computer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691b50c4",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Two different byte strings for the two characters\n",
    "\"\\u00C7\" == \"\\u0043\\u0327\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516b9002",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Unicode normalization\n",
    "\n",
    "In NER though, we will not want our NER model to interpret the two characters as two different characters. Therefore, we will need to first normalize them to make them the same at the byte level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcf8a37-22a7-47d2-a23a-06ab2f256f69",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import unicodedata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60460cb-e888-4b1b-812e-e024b0572596",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "| Name | Abbreviation | Description | Example |\n",
    "| --- | --- | --- | --- |\n",
    "| Form D | NFD | *Canonical* decomposition | `Ç` → `C ̧` |\n",
    "| Form C | NFC | *Canoncial* decomposition followed by *canonical* composition | `Ç` → `C ̧` → `Ç` |\n",
    "\n",
    "```\n",
    "Source: James Briggs - https://towardsdatascience.com/what-on-earth-is-unicode-normalization-56c005c55ad0\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7f1eba",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# compound C and accented C\n",
    "print(compound_c, accent_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be89fc2b",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"\\u00C7\", \"\\u0043\\u0327\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f901a3f4-1219-4581-aa9a-0fba005aaf48",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Decompose the accented character using Normal Form D\n",
    "nfd_accent = unicodedata.normalize('NFD', accent_c)\n",
    "print(compound_c == nfd_accent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9448ea24-7271-4443-a28a-ff02c01c001c",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Decompose and then compose using Normal Form C\n",
    "nfc_compound = unicodedata.normalize('NFC', compound_c)\n",
    "print(accent_c == nfc_compound)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7195523e",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Introduction to SpaCy\n",
    "The spaCy (spelled correctly) library is a robust library for Natural Language Processing. It supports a wide variety of languages with statistical models capable of parsing texts, identifying parts-of-speech, and extract entities. \n",
    "\n",
    "Let's see an example of NLP task that spaCy can do for us.\n",
    "\n",
    "## Tokenization\n",
    "Recall that last time we have seen a graph showing the NLP pipelines. A pipeline's purpose is to take input data, perform some sort of operations on that input data, and then output some useful information from the data. On a pipeline, we find the pipes. A pipe is an individual component of a pipeline. Different pipes perform different tasks. After we read in the data from a text file, an essential task of NLP is tokenization. \n",
    "\n",
    "<center><img src='./files/NER_NLP_pipeline.png' width=700></center>\n",
    "\n",
    "One form of tokenization is **word tokenization**. When we do word tokenization, we break a text up into individual words and punctuations. Another form of tokenization is **sentence tokenization**. Sentence tokenization is precisely the same as word tokenization, except instead of breaking a text up into individual words and punctuations, we break a text up into individual sentences.\n",
    "\n",
    "If you are an English speaker, you may think you do not need spaCy for sentence tokenization, because in English, the end of a sentence is indicated by a period `.`. Why not just use the the built-in `split()` function which allows us to split a text string by the period `.`? \n",
    "\n",
    "This is a ligit question, but simply splitting a text string by the period `.` will run into problems sometimes and spaCy is actually way more smarter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78e8d59-1320-4fe2-b52b-9d37806383ce",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "!python3 -m spacy download en_core_web_sm # for English NER\n",
    "!python3 -m spacy download es_core_news_sm # for Spanish NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6389d389-8901-4879-b640-e2da32912f6a",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# String to be split\n",
    "text = \"Martin J. Thompson is known for his writing skills. He is also good at programming.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0250ffb-e011-4a2f-a45d-fb1611de4e5e",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Martin J', ' Thompson is known for his writing skills', ' He is also good at programming', '']\n"
     ]
    }
   ],
   "source": [
    "# Split the string by period\n",
    "sents = text.split(\".\")\n",
    "print(sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696a8f57-fc45-4113-98b9-9b4e3f0a6a6c",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "We had the unfortunate result of splitting at Martin J. The reason for this is obvious. In English, it is common convention to indicate abbreviation with the same punctuation mark used to indicate the end of a sentence. \n",
    "\n",
    "We can use SpaCy, however, to do sentence tokenization. SpaCy is smart enough to not break at Martin J."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8a98ae-02bb-4c01-8753-88d8972b6226",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "First, let's import the spaCy library. Then, we need to load an NLP model object. To do this, we use the `spacy.load()` function. Here, we load the small English NLP model trained on written web text that includes vocabulary, syntax and entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9faa2808-b8b7-4319-91b0-0dee5b8ea29b",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Load the small English NLP model\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2213db29-affd-40fb-a600-790aee23a940",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "We can use this English NLP model to parse a text and create a Doc object. If you need a quick refresh about what classes and object are, you can refer to [Python intermediate 4](../Python-intermediate/python-intermediate-4.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537cd415-512d-47bf-9562-e9befbae5a5f",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Use the English model to parse the text we created\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b61756-409c-4bb8-a60f-1faecd4ce728",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "There is a lot of data stored in the Doc object. For example, we can iterate over the sentences in the Doc object and print them out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3a2fba-390b-43f7-88ab-6a8b5b41189a",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Get the sentence tokens in doc\n",
    "for sent in doc.sents:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7826cd-242e-4e36-88b0-2886cf686241",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# spaCy's built-in NER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f8a2c0-0cd2-4503-a27d-f3793b843512",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "We have seen one example NLP task that spaCy can do for us. Now let's move on to named entity recognition, the NLP task we focus on in this series.\n",
    "\n",
    "SpaCy already has a built-in NER off the shelf for us to use. \n",
    "\n",
    "We will iterate over the doc object as we did above, but instead of iterating over `doc.sents`, we will iterate over `doc.ents`. For our purposes right now, we simply want to get each entity's text (the string itself) and its corresponding label (note the underscore `_` after label)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7e6829-887c-4110-ae7e-5239fd4338bd",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Print out the entities in the doc object together with their labels\n",
    "for ent in doc.ents: # iterate over the entities \n",
    "    print (ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2992c0fe-f8e7-41cf-bb14-4845444068ed",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "As we can see the small English model has correctly identified that Martin J. Thompson is an entity and given it the correct label PERSON."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881674cb-9db2-4b31-a2fa-a5c6bcb7307e",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Of course we have many different kinds of entities. Here is a list of entity labels used by the small English NLP model we loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92202131-518c-451e-bb41-20c5296c60da",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# List of labels in the small English model for NER\n",
    "nlp.get_pipe(\"ner\").labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0799e83-3d10-43cd-8c37-258f98e427bd",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "If you would like to know the meaning of a label, you can use the `explain` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5383e1a-23a3-4e42-971c-b3e342577f8f",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Get what a label means\n",
    "spacy.explain('NORP')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9bb6bf4-d53e-4548-9cbf-9315e686ff42",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "<h2 style=\"color:red; display:inline\">Coding Challenge! &lt; / &gt; </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99758079-7898-4b21-ac8b-ec4a655cfb61",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "### Get the .csv file for this exercise\n",
    "\n",
    "hp_file = './files/NER_Harry_Potter_1.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9d05a5-60e8-4899-936f-0f167e283d71",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "### Take a look at the first five rows of the table\n",
    "import pandas as pd\n",
    "df = pd.read_csv(hp_file, delimiter=';')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7d1870-219b-4063-a5f5-772a0021b9eb",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "In this table we find the name of the characters speaking and their speech. \n",
    "\n",
    "Can you make two new columns, \"Entities\" and \"Labels\", such that each row of the \"Entities\" column stores a list of entities found in the sentence in the same row and each row of the \"Labels\" column stores a list of labels for the entities?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce40eac-ce43-468e-bc9c-22a0942fdf74",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "32668c03-3339-40a8-a458-a34f46a79595",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# spaCy's EntityRuler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762a6c7f-d09f-4ea7-b6bd-46f911db5e89",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Life would be so easy if we could just grab the ready-to-use built-in NER of spaCy and apply it to the large volume of data we have at hand. However, things are not that easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727bccab-28b8-42ee-8836-78b6e50d728c",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Another sample text string\n",
    "text = \"Aars is a small town in Denmark. The town was founded in the 14th century.\"\n",
    "\n",
    "#Create the Doc object\n",
    "doc = nlp(text)\n",
    "\n",
    "#extract entities\n",
    "for ent in doc.ents:\n",
    "    print (ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691ac8c5-e18d-4508-b4d6-67e44b98f94d",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "We see that the built-in NER failed to identify Aars as an entity of the GPE type. If we do want to extract 'Aars' from the text and give it a label of GPE, what can we do? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b10ffd0-58f3-4989-84c3-3896a2e451f0",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Add EntityRuler as a new pipe\n",
    "\n",
    "Recall that we have talked about the pipes in a pipeline at the beginning of this lesson. In the case of spaCy, there are a few different pipes that perform different tasks. The tokenizer tokenizes the text into individual tokens; the parser parses the text, and the NER identifies entities and labels them accordingly. When we create a Doc object, all of this data is stored in the Doc object.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd25250-9fba-46b8-8a9c-ad079c00c652",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Take a look at the current pipes\n",
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf02644f-abd0-4715-9972-ca3fb78ba7f2",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "The EntityRuler is a spaCy factory that allows one to create a set of patterns with corresponding labels. In order to extract the target entities and label them successfully, we can create an EntityRuler, give it some instructions, and then add it to the spaCy pipeline as a new pipe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a1414d-600d-4b8d-9568-aab215600834",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Create the EntityRuler\n",
    "ruler = nlp.add_pipe(\"entity_ruler\")\n",
    "\n",
    "# List of Entities and Patterns\n",
    "patterns = [\n",
    "                {\"label\": \"GPE\", \"pattern\": \"Aars\"}\n",
    "            ]\n",
    "\n",
    "ruler.add_patterns(patterns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e968631-3ee2-4d3f-9397-b5189041f99d",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "After we add the EntityRuler, we can use the new pipeline to do NER. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9adc414-53a0-4912-8416-8d57c142ac0e",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Use the new model to parse the text and create a new Doc object\n",
    "doc = nlp(text)\n",
    "\n",
    "# Iterate over the entities and print them out\n",
    "for ent in doc.ents: \n",
    "    print (ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e8e61d-6bd5-4498-8c15-b638140d7d8a",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Take a look at the pipes in the new pipeline\n",
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e8cd59-6f5c-49e0-9162-1ca1c11e7537",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## The importance of order"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da659ace-0069-452e-8980-94fb04d91c02",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "It is important to remember that pipelines are sequential. This means that components earlier in a pipeline affect what later components receive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ab5971-fbcb-4e57-982c-6b7c7a7f34fc",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Use the new model to parse a new text string\n",
    "text = \"Xiong'an is a satellite city of Beijing.\"\n",
    "nlp1 = spacy.load(\"en_core_web_sm\")\n",
    "doc=nlp1(text)\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817a5685-d65c-4060-838a-6a3fd6cf9bf7",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Xiong'an is a name of a city. We would want to label it as GPE, not ORG. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f4b14e-661a-41ff-b0c7-93e67b9c5887",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Create the EntityRuler\n",
    "ruler = nlp1.add_pipe(\"entity_ruler\")\n",
    "\n",
    "# List of Entities and Patterns\n",
    "patterns = [\n",
    "                {\"label\": \"GPE\", \"pattern\": \"Xiong'an\"}\n",
    "            ]\n",
    "\n",
    "ruler.add_patterns(patterns)\n",
    "\n",
    "# Get the entities\n",
    "doc = nlp1(text)\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f895bfe6-4ea4-4873-80e9-487a91737da9",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Why do we still mislabel Xiong'an? This is because when we add the EntityRuler as a new pipe, it gets added at the end of the pipeline automatically. That means the EntityRuler will come after the built-in NER in spaCy. Since NER is a hard classification task, an entity that gets labeled will not be relabeled. If Xiong'an is labeled already by the built-in NER as ORG, it will not be relabeled by the EntityRuler that comes after. In order to give the EntityRuler primacy, we will have to put it in a position before the built-in NER when we add it so that it takes primacy over the built-in NER. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff94e6f5-ba7e-4f44-b97e-f19f569c32e6",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Load the model\n",
    "nlp2 = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Create the EntityRuler and add it to the model\n",
    "ruler = nlp2.add_pipe(\"entity_ruler\", before='ner') # specify that the EntityRuler comes before built-in NER\n",
    "\n",
    "# Add the new patterns to the ruler\n",
    "patterns = [\n",
    "                {\"label\": \"GPE\", \"pattern\": \"Xiong'an\"}\n",
    "            ]\n",
    "\n",
    "ruler.add_patterns(patterns)\n",
    "\n",
    "# Use the new model to parse the text\n",
    "doc = nlp2(text)\n",
    "\n",
    "# Get the entities\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff91edad-202e-4ea7-964b-7a8ccf031eeb",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# EntityRuler comes before the built in ner in nlp2\n",
    "nlp2.pipe_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b347d63-166b-4ed0-bf34-8e9a9aa68ad2",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "So far, we only add exact strings to our EntityRuler. However, when we talk about patterns, we usually talk about more abstract patterns, not fixed strings. In the following, we will see an example where we write a regular expression pattern and add it to the EntityRuler."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7393d6ee-22f3-4e9c-a0ac-671e045f440f",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Write a regex pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f1677a-706e-462c-9080-78fc3576e549",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Suppose we have a text written in English, except that the names are written in Latin. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca969215-74ae-4faa-aaae-d2e11cad2196",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# English text with Latin names\n",
    "text = \"Marius was a consul in Rome. Marie is the vocative form.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d58324-9a6a-456d-b910-916360926985",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "We could write a function that captures the different forms of the name Marius."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f81c90-f0de-4c8f-a35d-9ac25591007a",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Write a function that captures the pattern for the Latin name Marius\n",
    "def pattern(root):\n",
    "    endings = [\"us\", \"i\", \"o\", \"um\", \"e\"] # the different endings of the name \n",
    "    patterns = [{\"label\": \"PERSON\", \"pattern\": root+ending} for ending in endings]\n",
    "    return patterns\n",
    "marius = pattern(\"Mari\")\n",
    "marius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbe94e0-6883-407e-8511-49fc08e8e821",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Create an empty English NLP model\n",
    "nlp_latin = spacy.blank(\"en\")\n",
    "\n",
    "# Add an EntityRuler\n",
    "nlp_latin_ruler = nlp_latin.add_pipe(\"entity_ruler\")\n",
    "\n",
    "# add the pattern for the Latin name Marius to the EntityRuler\n",
    "nlp_latin_ruler.add_patterns(marius)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e31ec75-2252-4d57-9dcc-3c7b9a4cb099",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Create a Doc object\n",
    "doc_latin = nlp_latin(text)\n",
    "\n",
    "# Iterate over the entities in Doc object and print them out\n",
    "for ent in doc_latin.ents:\n",
    "    print (ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23366537-6466-40dc-82d6-9d37d3041029",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "We can also use regular expressions (regex) to specify the pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c250f1b2-b383-414d-a2fd-f6f915013417",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Write a function which returns the pattern for Latin name Marius\n",
    "def latin_roots(root):\n",
    "    return [{\"label\": \"PERSON\", \"pattern\": [{\"TEXT\": {\"REGEX\": \"^\" + root + r\"(us|i|o|um|e)$\"}}]}]\n",
    "\n",
    "# Save the pattern to the variable marius2\n",
    "marius2 = latin_roots(\"Mari\")\n",
    "\n",
    "# Create a blank English NLP model\n",
    "nlp_latin2 = spacy.blank(\"en\")\n",
    "\n",
    "# Add an EntityRuler to the model\n",
    "nlp_latin_ruler2 = nlp_latin2.add_pipe(\"entity_ruler\")\n",
    "\n",
    "# Add the pattern for Latin name Marius to the EntityRuler\n",
    "nlp_latin_ruler2.add_patterns(marius2)\n",
    "\n",
    "# Text to be parsed\n",
    "text = \"Marius was a consul in Rome. Marie is the vocative form. Caesar was a dictator.\"\n",
    "\n",
    "# Create a Doc object using the new model with the regex pattern in EntityRuler\n",
    "doc_latin2 = nlp_latin2(text)\n",
    "\n",
    "# Iterate over the entities and print them out\n",
    "for ent in doc_latin2.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27063f52-0b62-4a96-916b-b695ece71545",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "<h2 style=\"color:red; display:inline\">Coding Challenge! &lt; / &gt; </h2>\n",
    "\n",
    "You have seen in coding challenge one that the off-the-shelf NER of Spacy mislabeled some entities. For example, \"Hagrid\", a person's name, is labeled as ORG. Suppose you have a file with all the characters' names in it. Can you make an EntityRuler and add it to the SpaCy pipeline so that all the person names will be labeled 'PERSON'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1fe092-388f-43bf-83d5-52796b426385",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "### Get the .csv file for this exercise\n",
    "hp_file = './files/NER_HarryPotter_Characters.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612d9f1f-70f1-4208-a882-9dcfa36d7b84",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Read in the data from the character csv file\n",
    "chars_df = pd.read_csv(hp_file, delimiter=';')\n",
    "\n",
    "# Take a look at the first five rows\n",
    "chars_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ebbf62-8149-4852-a021-60fac371ca55",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Get all parts from a character's name\n",
    "chars_df = chars_df[['Name']] \n",
    "chars_df['split_name'] = chars_df['Name'].str.split(' ')\n",
    "chars_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f69659-03b2-47be-8284-ebbb4b24f437",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Get the first names and last names of the characters\n",
    "chars_df['first_name'] = chars_df['split_name'].str[0]\n",
    "chars_df['last_name'] = chars_df['split_name'].str[-1]\n",
    "\n",
    "first_names = chars_df['first_name'].unique().tolist() # Put all unique first names in a list\n",
    "last_names = chars_df['last_name'].unique().tolist() # Put all unique last names in a list\n",
    "\n",
    "names = list(set(first_names) | set(last_names)) # the vertical bar | gives us the union of the two sets\n",
    "\n",
    "names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01fea3be-0bdb-4758-b57b-7994f78b1b67",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Create an EntityRuler. In the ruler, add all characters' names as pattern and specify the label for them as \"PERSON\". Add the ruler as a new pipe. Last, add two new columns to the dataframe you created from the original NER_Harry_Potter_1.csv file, one storing the entities found in each sentence and one storing the labels for the entities. This time, all characters' names should be correctly labeled as \"PERSON\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac21077c-0403-4371-b516-c90f1dae60ff",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4a8c80b1-4b5d-4b77-907d-2287908ac7e9",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Detecting languages in texts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9948c696-b54c-453d-a6d9-c9d924c38baf",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "When we work with a multilingual corpus, we will first want to know the different languages used in the corpus. There are different approaches to do this. In this section, I will introduce a third-party library Lingua for language detection. Currently, 75 languages are supported by Lingua. Lingua is an open-source project and the github repository for Lingua is here https://github.com/pemistahl/lingua-py.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb27c97-903b-418f-bd03-9d0c8f281816",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Language detection with Lingua"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c47545-6d3b-4745-9bc4-5e3ff0b6f69d",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Install language detector\n",
    "\n",
    "!pip3 install lingua-language-detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75dd0dd-3bfb-40b9-b0cd-81595bce5a92",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# import the language detector builder\n",
    "from lingua import LanguageDetectorBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e468a2d6-bb8e-4a18-9834-a54f57f0bc21",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# build a language detector\n",
    "detector = LanguageDetectorBuilder.from_all_languages().build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50d1d49-0edb-41b0-b597-c5d92f174a2d",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Use the detector to detect the language of a string\n",
    "detector.detect_language_of(\"This is an English text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716a16e4-bc59-40b9-89b1-ba724c26dc84",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Use the detector to detect the language of a string\n",
    "detector.detect_language_of(\"Este é um outro texto sem idioma especificado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14ec85f-5a4c-4e0f-964f-aa6d07a3e87c",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Use the detector to detect the language of a string\n",
    "detector.detect_language_of(\"这是一句中文\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d9bedf-006f-4825-81a6-35911efdd516",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Sometimes you may already know the range of languages in your corpus. You just want to identify the language for each document. In this case, you could narrow down the language detector to only a few languages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6a74e3-2633-482f-9267-338bc7781745",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# build a language detector\n",
    "from lingua import Language, LanguageDetectorBuilder\n",
    "languages = [Language.ENGLISH, Language.FRENCH, Language.GERMAN, Language.SPANISH]\n",
    "detector = LanguageDetectorBuilder.from_languages(*languages).build()\n",
    "\n",
    "# Use the detector to decide between the given languages \n",
    "detector.compute_language_confidence_values(\"This is an English text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98db87a-f4d8-4849-936e-76366386a5bc",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Multiple languages in the same file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e341f58-1513-4c2f-8ca3-fdc9c940e912",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "The examples we go over just now assume that only one language is used in each document. However, the language detector we build cannot reliably detect multiple languages, because it will only output one language for a text by default. What if our text has multiple languages, such as the example below?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1dcbf6-9206-4c9e-8458-f6e20b735a9f",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# a text string with multiple languages \n",
    "large_text = '''This is a text where the first line is in English.\n",
    "Maar de tweede regel is in het Nederlands. \n",
    "Dies ist ein deutscher Text.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1c6f7b-7a52-4a88-b376-84ddbfbf25ff",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# build a language detector\n",
    "languages = [Language.ENGLISH, Language.DUTCH, Language.GERMAN]\n",
    "detector = LanguageDetectorBuilder.from_languages(*languages).build()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e642b051-1d91-4d8e-911f-98f4949f2420",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "If we run the detector over this text, we get the following output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a26b64-94d3-4fb6-85cb-9fb5e04b30b2",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Use the detector to decide the language of the text\n",
    "detector.detect_language_of(large_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31805e20-06ac-4f61-b01f-91636fbafa23",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "By default, Lingua returns the most likely language for a given input text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997b6426-5902-4b43-9908-a24f0fc03a34",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Get the likelihood of the decision\n",
    "confidence_values = detector.compute_language_confidence_values(large_text)\n",
    "for confidence in confidence_values:\n",
    "    print(f\"{confidence.language.name}: {confidence.value:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f55b12-368d-453e-8e0b-f060abc6d961",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "But this text has multiple languages. In this example text, each sentence is written in a different language. Therefore, we need to get each sentence string and run the detector over it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed35fca-01e4-4c09-8454-24e280241854",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Create a Doc object \n",
    "doc = nlp(large_text)\n",
    "\n",
    "# Iterate over each sentence and run the detector over it\n",
    "for sent in doc.sents:\n",
    "    print(f\"Sentence: {sent.text.strip()}\")\n",
    "    print(detector.detect_language_of(sent.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7069919-00a6-4061-b5f2-e35cc6846f84",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Bring everything together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3f5d52-ea44-4aa0-878d-f67364eb0de9",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# A document that has two languages, English and Spanish\n",
    "multilingual_document = \"\"\"This is a story about Margaret who speaks Spanish. \n",
    "'Juan Miguel es mi amigo y tiene veinte años.' Margeret said to her friend Sarah.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727a5d24-5540-4f5f-9f8b-5482ba3feb13",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# build a language detector\n",
    "from lingua import Language, LanguageDetectorBuilder\n",
    "languages = [Language.ENGLISH, Language.FRENCH, Language.GERMAN, Language.SPANISH]\n",
    "detector = LanguageDetectorBuilder.from_languages(*languages).build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf993a01-0652-4db0-a8a4-a1baaac40e04",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Load the relevant models\n",
    "english_nlp = spacy.load(\"en_core_web_sm\") # for English\n",
    "spanish_nlp = spacy.load(\"es_core_news_sm\") # for Spanish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f65929-fa3f-4225-b0a4-22407b77bce1",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Create an NLP model and create a Doc object\n",
    "multi_nlp = spacy.blank('en')\n",
    "\n",
    "# Add sentencizer\n",
    "multi_nlp.add_pipe('sentencizer')\n",
    "\n",
    "# Create a Doc object\n",
    "multi_doc = multi_nlp(multilingual_document.strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a954a1f9-4a7a-4ea5-9273-5ca080422c5f",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Switching between languages with conditionals\n",
    "\n",
    "for sent in multi_doc.sents:\n",
    "    if detector.detect_language_of(sent.text).name == \"ENGLISH\":\n",
    "        nested_doc = english_nlp(sent.text.strip())\n",
    "    elif detector.detect_language_of(sent.text).name == \"SPANISH\":\n",
    "        nested_doc = spanish_nlp(sent.text.strip())\n",
    "    for ent in nested_doc.ents:\n",
    "        print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031ec5c3-3420-4dc5-826c-3e9547c325b1",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "!python3 -m spacy download en_core_web_sm # for English NER\n",
    "!python3 -m spacy download en_core_web_md # for showing the word vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25af6ee-6dee-4d06-93d1-885af11ce372",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Introduction to word embeddings\n",
    "\n",
    "How do we represent word meanings in NLP? One way we can represent word meanings is to use word vectors. **Word embeddings** are vector representations of words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16a9b05-1e0c-4473-9a93-d2d924654ece",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Distributional hypothesis\n",
    "\n",
    "Word embeddings is inspired by the **distributional hypothesis** proposed by Harris ([1954](https://link.springer.com/chapter/10.1007/978-94-009-8467-7_1). This theory could be summarized as: words that have similar context will have similar meanings.\n",
    "\n",
    "What does \"context\" mean in word embeddings? Basically, \"context\" means the neighboring words of a target word. \n",
    "\n",
    "Consider the following example. If we choose \"village\" as the target word and choose a fixed size context window of 2, the two words before \"village\" and the two words after \"village\" will constitute the context of the target word.\n",
    "\n",
    "Treblinka is **a small** **<span style=\"color: blue;\">village</span>** **in Poland.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9575bf7c-342f-4d18-ac53-b4c00f362bf5",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Word2Vec\n",
    "\n",
    "Google’s pre-trained word2vec model includes word vectors for a vocabulary of 3 million words and phrases that they trained on roughly 100 billion words from a Google News dataset. The vector length is 300 features, which means each of the 3 million words in the vocabulary is represented by a vector with 300 floating numbers. Word2Vec is one of the most popular techniques to learn word embeddings.\n",
    "\n",
    "The training samples are the (target, context) pairs from the text data. For example, suppose your source text is the sentence \"The quick brown fox jumps over the lazy dog\". If you choose \"quick\" as your target word and have set a context window of size 2, you will get three training samples for it, i.e. (quick, the), (quick, brown) and (quick fox).   \n",
    "\n",
    "**McCormick, C**. (2016, April 19). Word2Vec Tutorial - The Skip-Gram Model. Retrieved from http://mccormickml.com/\n",
    "\n",
    "The word2vec model is trained to accomplish the following task: given the input word $w_{1}$, for each word $w_{2}$ in our vocab, how likely $w_{2}$ is a context word of $w_{1}$.\n",
    "\n",
    "The network is going to learn the statistics from the number of times each (target, context) shows up. So, for example, if you have a text about kings, queens and kingdoms, the network is probably going to get many more training samples of (\"King\", \"Queen\") than (\"King\", \"kangaroo\"). Therefore, if you give your trained model the word \"King\" as input, then it will output a much higher probability for \"Queen\" than it will for \"kangaroo\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054e8468-e69f-4abe-88b3-92282e29aa67",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Word vectors in SpaCy\n",
    "\n",
    "We have used the small English model from spaCy in the previous two notebooks. Actually, there are medium size and large size English models from spaCy as well. Both are trained using the word2vec family of algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33f9e44-58ca-4664-be13-94cc7e3f6e10",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the medium size English model from spaCy\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "\n",
    "# Get the word vector for the word \"King\"\n",
    "nlp(\"King\").vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a24a95-2246-40a5-a47e-c3e917a533fd",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Get the size of the vector\n",
    "nlp(\"King\").vector.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338ea8d2-d30a-4d09-bd52-0713ae560c6f",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Get the similarity between the two words \"King\" and \"Queen\"\n",
    "nlp(\"King\").similarity(nlp(\"Queen\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49deed3-d015-49c8-ba19-9dea89269de7",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Get the similarity between the two words \"King\" and \"kangaroo\"\n",
    "nlp(\"King\").similarity(nlp(\"kangaroo\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1118f693-f4b5-4b2d-8fe5-b446a624caa9",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Word2Vec\n",
    "\n",
    "The Word2Vec model is a machine learning model. This means that it is not based on explicitly defined rules, but derived from statistical data: instead of trying to come up with exceptionless rules, we use data to approximate them.\n",
    "Let's use a simple example to understand the ML pipeline. Suppose you are interested in the relationship between the size and the price of a house in your neighborhood. Specifically, you would like to use the size of a house to predict its price. You collect information about the recently sold houses in your neighborhood. You note down their size and sale price. You draw a scatter plot like the following to examine the data. \n",
    "\n",
    "<center><img src='./files/NER_housebuying_scatter.png' width=300></center>\n",
    "\n",
    "Now, you would like to derive a relationship between the house size and house price. Essentially, you fit a line to the data points. Your machine learning technique determines which line you end up with - you can use for example linear regression.\n",
    "\n",
    "<center><img src='./files/NER_housebuying.png' width=300></center>\n",
    "\n",
    "The function for this line is y = ax + b (where y is the price and x is the # of sqft). Of course, you would not just fit any line to your data points. You would want to fit a line so that the difference between the actual house prices and the predicted house prices is the smallest. Our task, then, reduces to the calculation of the value of a and b in the function y = ax + b so that the difference between the actual house prices and the predicted house prices is the smallest.\n",
    "\n",
    "The ML method used in word2vec is more sophisticated. It is a shallow neural network with one hidden layer of neurons and one output layer of neurons. Chris McCormick has a very detailed explanation of this model in his blog post http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/. Despite this being a 'neural network' model, it is not a *deep learning* model (hence *shallow*) where the resulting behaviour gets much harder to interpret."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436c45b6-bce3-4ce5-a240-0caadcbe9134",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Training and evaluation\n",
    "\n",
    "Trained models should hold more than a memory of seen cases: the purpose is to build useful generalizations that work also for new data. Learning that “Amazon” right here is a company is not enough; it should derive from the context in which the word appears that \"Amazon\" is probably a company. But this is why the selection of training data matters: training a model on Wikipedia will give you much fewer sentences in the first person than on Twitter. Training on Wikipedia and then trying to analyse data from Twitter can therefore lead to problems.\n",
    "\n",
    "In practice, to avoid 'overfitting' (good results in training but poor generalization), we separate the annotated samples into training and testing data. \n",
    "\n",
    "https://spacy.io/usage/training\n",
    "\n",
    "The training data is used to hone a statistical model via predetermined algorithms. It does this by making guesses about what the proper labels are. It then checks its accuracy against the correct labels, i.e., the annotated labels, and makes adjustments accordingly. Once it is finished viewing and guessing across all the training data, the first **epoch**, or **iteration** over the data, is finished. At this stage, the model then tests its accuracy against the evaluation data. The training data is then randomized and given back to the system for x number of epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2251c2-76b5-45e2-b3ac-7ba30be77dfe",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# NER with EntityRuler vs. ML NER\n",
    "\n",
    "In this section, we are going to make two models to do the same NER task, one doing NER with an EntityRuler and the other doing NER using word vectors.\n",
    "\n",
    "First, let's download the two data files needed for this example. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc982dbc-c485-47df-9c64-9b0f840dd870",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "The first file stores the information about the spells in Harry Potter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0713d496-cf7a-480f-8230-dc90ee8a5e4d",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "hp_spells = './files/NER_HarryPotter_Spells.csv'\n",
    "spells_df = pd.read_csv(hp_spells, sep=\";\")\n",
    "spells_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ab680c-a57e-45e4-848d-65e366abdbd0",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "In the second file, we find the characters speaking and their speech. Notice that there is a column storing the spells found in the sentence if there is one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69122f6-f5d8-415c-a863-570fa9282131",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "hp_film_spells = './files/NER_HarryPotter_FilmSpells.csv'\n",
    "film_spells = pd.read_csv(hp_film_spells)\n",
    "film_spells"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732afc01-dee8-4e88-a70b-4fb4d03d2478",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Suppose we would like to create a model that can identify spells in a sentence and give it the label 'SPELL'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4568f93-a5f4-4015-b2a2-b304216bca2a",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Create an NLP model with an EntityRuler to identify the spells\n",
    "\n",
    "In the following, we will first create a NLP model with an entity ruler that identifies spells. This section can be seen as a review of what we have learned about EntityRuler in Wednesday's lesson.\n",
    "Before we create a new EntityRuler, we will do some preprocessing of the data to get the patterns that we will add to the EntityRuler."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0b2919-a69f-4b2a-901a-15447ed57b82",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23dd6ab2-981b-40da-b198-da511121c609",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Fill the NaN cells with an empty string\n",
    "spells_df['Incantation'] = spells_df['Incantation'].fillna(\"\")\n",
    "\n",
    "# Get all spells\n",
    "spells = spells_df['Incantation'].unique().tolist() # Put all strs in the 'Incantation' column in a list\n",
    "spells = [spell for spell in spells if spell != ''] # Get all non-empty strs from the list, i.e. all the spells\n",
    "\n",
    "# Take a look at the spells\n",
    "spells"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6da736f-af77-4734-ba64-4d44ce961694",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Creating the patterns to be added to the EntityRuler\n",
    "\n",
    "Recall from Wednesday's lesson that the patterns we add to an EntityRuler look like the following.\n",
    "\n",
    "`patterns = [{\"label\": \"GPE\", \"pattern\": \"Aars\"}]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfecd337-82c3-48f5-b4e4-5a41cb133637",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Write the pattern to be added to the ruler\n",
    "patterns = [{\"label\":\"SPELL\", \"pattern\":spell} for spell in spells]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0783452f-5b0e-41e5-986e-cdc8a2b33f69",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Now that we have the patterns ready, we can add them to an EntityRuler and add the ruler as a new pipe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc33172a-53b8-49f0-8860-d12aa538080e",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Create an EntityRuler and add the patterns to the ruler\n",
    "entruler_nlp = spacy.blank('en') # Create a blank English model\n",
    "ruler = entruler_nlp.add_pipe(\"entity_ruler\") \n",
    "ruler.add_patterns(patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae88ae3e-ca49-46cc-b82f-897390275ca5",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "test_text = \"\"\"Ron Weasley: Wingardium Leviosa! Hermione Granger: You're saying it wrong. \n",
    "It's Wing-gar-dium Levi-o-sa, make the 'gar' nice and long. \n",
    "Ron Weasley: You do it, then, if you're so clever\"\"\"\n",
    "doc = entruler_nlp(test_text)\n",
    "for ent in doc.ents:\n",
    "    print('EntRulerModel', ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6e24bd-64eb-460a-b7b7-15b91402cd93",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "In this model, we have basically hard written all spell strings in the EntityRuler. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99914cb7-423e-4df6-ab3f-14686122c069",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Train a NLP model using ML to identify the spells"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559d3b87-8743-4d43-a299-386e2c8d5bde",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "The format of the training data will look like the following. It is a list of tuples. In each tuple, the first element is the text string containing spells and the second element is a dictionary. The key of the dictionary is 'entities'. The value is a list of lists. In each list, we find the starting index, ending index and the label of the spell(s) found in the text string. \n",
    "\n",
    "`[\n",
    "('Oculus Reparo', {'entities': [[0, 13, 'SPELL']]}),\n",
    "('Alohomora', {'entities': [[0, 9, 'SPELL']]})\n",
    "]`\n",
    "\n",
    "The text strings we use for the training are from the 'Sentence' column of the film_spells dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef124206-0fb4-4439-a152-8a30988998eb",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Take a look at the film_spells df\n",
    "film_spells"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fdc2c9a-9130-4021-ba1c-89b836d48c87",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Since we have hard written all spell strings in the EntityRuler and give them the label 'SPELL', we could just use this model to generate labeled data as our training data and evaluation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fd821f-d57f-440f-90c5-5be51ae16d8a",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import nltk \n",
    "nltk.download('punkt')\n",
    "# for sentence tokenization\n",
    "nltk.download('punkt_tab')\n",
    "def generate_labeled_data(ls_sents): # the input will be a list of strings\n",
    "    text = ' '.join(ls_sents)\n",
    "    sents = nltk.sent_tokenize(text)\n",
    "    labeled_data = []\n",
    "    for sent in sents:\n",
    "        doc = entruler_nlp(sent) # create a doc object\n",
    "        if doc.ents != (): # if there is at least one entity identified\n",
    "            labeled_data.append((sent, {\"entities\":[[ent.start_char, ent.end_char, ent.label_] for ent in doc.ents]}))\n",
    "    return labeled_data       \n",
    "\n",
    "# Assign the result from the function to a new variable\n",
    "training_validation_data = generate_labeled_data(film_spells['Sentence'].tolist())\n",
    "\n",
    "# Take a look at the labeled data\n",
    "training_validation_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107ca0bf-d972-4807-a157-926100ce8d7b",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "spaCy 3 requires that our data be stored in the proprietary `.spacy` format. To do that we need to use the `DocBin` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d240c25-9b6b-4fcb-b2c5-1d6c590c3007",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from spacy.tokens import DocBin \n",
    "\n",
    "db = DocBin() \n",
    "\n",
    "for text, annot in training_validation_data[:19*2]: # Get the first 38 tuples as the training data\n",
    "    doc = entruler_nlp(text) # create a doc object\n",
    "    doc.ents = [doc.char_span(ent[0], ent[1], label=ent[2]) for ent in annot['entities']]\n",
    "    db.add(doc)\n",
    "db.to_disk(f\"./train_spells.spacy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a153e58-0bcc-4dc2-985b-134095ec0141",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "for text, annot in training_validation_data[19*2:]: # Get the rest tuples as the validation data\n",
    "    doc = entruler_nlp(text) \n",
    "    doc.ents = [doc.char_span(ent[0], ent[1], label=ent[2]) for ent in annot['entities']]\n",
    "    db.add(doc)\n",
    "db.to_disk(f\"./valid_spells.spacy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3778c726-4130-44e1-8650-f16b14c5002f",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Now we can finally start training our model! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffc3e7a-4801-4822-a97f-42eb8f2461e5",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "!python3 -m spacy init config --lang en --pipeline ner config.cfg --force"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af27eb3b-3ce6-4d98-9df8-04d16dfe3229",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "!python3 -m spacy train config.cfg --output ./output/spells-model/ --paths.train ./train_spells.spacy --paths.dev ./valid_spells.spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184bce47-770f-48a6-bbea-63a9a47a4d15",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Now let's finally run our model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5983e5-11d9-45f5-ab5e-cbc223106745",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Load the best model\n",
    "model_best = spacy.load('./output/spells-model/model-best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a23a63-5857-49c2-a732-8e799c35d1bf",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Let's try our model on this long text string\n",
    "test_text = \"\"\"53. Imperio - Makes target obey every command But only for really, really funny pranks. 52. Piertotum Locomotor - Animates statues On one hand, this is awesome. On the other, someone would use this to scare me.\n",
    "\n",
    "51. Aparecium - Make invisible ink appear\n",
    "\n",
    "Your notes will be so much cooler.\n",
    "\n",
    "50. Defodio - Carves through stone and steel\n",
    "\n",
    "Sometimes you need to get the eff out of there.\n",
    "\n",
    "49. Descendo - Moves objects downward\n",
    "\n",
    "You'll never have to get a chair to reach for stuff again.\n",
    "\n",
    "48. Specialis Revelio - Reveals hidden magical properties in an object\n",
    "\n",
    "I want to know what I'm eating and if it's magical.\n",
    "\n",
    "47. Meteolojinx Recanto - Ends effects of weather spells\n",
    "\n",
    "Otherwise, someone could make it sleet in your bedroom forever.\n",
    "\n",
    "46. Cave Inimicum/Protego Totalum - Strengthens an area's defenses\n",
    "\n",
    "Helpful, but why are people trying to break into your campsite?\n",
    "\n",
    "45. Impedimenta - Freezes someone advancing toward you\n",
    "\n",
    "\"Stop running at me! But also, why are you running at me?\"\n",
    "\n",
    "44. Obscuro - Blindfolds target\n",
    "\n",
    "Finally, we don't have to rely on \"No peeking.\"\n",
    "\n",
    "43. Reducto - Explodes object\n",
    "\n",
    "The \"raddest\" of all spells.\n",
    "\n",
    "42. Anapneo - Clears someone's airway\n",
    "\n",
    "This could save a life, but hopefully you won't need it.\n",
    "\n",
    "41. Locomotor Mortis - Leg-lock curse\n",
    "\n",
    "Good for footraces and Southwest Airlines flights.\n",
    "\n",
    "40. Geminio - Creates temporary, worthless duplicate of any object\n",
    "\n",
    "You could finally live your dream of lying on a bed of marshmallows, and you'd only need one to start.\n",
    "\n",
    "39. Aguamenti - Shoot water from wand\n",
    "\n",
    "No need to replace that fire extinguisher you never bought.\n",
    "\n",
    "38. Avada Kedavra - The Killing Curse\n",
    "\n",
    "One word: bugs.\n",
    "\n",
    "37. Repelo Muggletum - Repels Muggles\n",
    "\n",
    "Sounds elitist, but seriously, Muggles ruin everything. Take it from me, a Muggle.\n",
    "\n",
    "36. Stupefy - Stuns target\n",
    "\n",
    "Since this is every other word of the \"Deathly Hallows\" script, I think it's pretty useful.\"\"\"\n",
    "\n",
    "# Create a doc object out of the text string using the trained model\n",
    "doc = model_best(test_text)\n",
    "\n",
    "# Find out the entities\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19d131f-d92b-4b38-9847-eb8b4ba89d08",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Let's also try the model we created with an EntityRuler with all spell names hard written in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab399e5-1912-46c7-875a-01b550a78bf1",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Create a doc object out of the text string using the EntityRuler model\n",
    "doc = entruler_nlp(test_text)\n",
    "\n",
    "# Find out the entities\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc2be64-2aa4-4c10-857f-acce6be87f3b",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "It seems in this example our EntityRuler model performs better than our trained model. Why do we think that is?\n",
    "\n",
    "Part of the reason we aren't getting better results is something that Ines Montani describes in this Stack Overflow answer https://stackoverflow.com/questions/50580262/how-to-use-spacy-to-create-a-new-entity-and-learn-only-from-keyword-list/50603247#50603247\n",
    "\n",
    "\"The advantage of training the named entity recognizer to detect SPECIES in your text is that the model won't only be able to recognise your examples, but also generalise and recognise other species in context. If you only want to find a fixed set of terms and not more, a simpler, rule-based approach might work better for you.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a4c8e6-1a68-433f-9b99-fc23ac22def2",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# References\n",
    "McCormick, C. (2016, April 19). Word2Vec Tutorial - The Skip-Gram Model. Retrieved from http://www.mccormickml.com\n",
    "\n",
    "Bird, S., Klein, E., & Loper, E. (2009). Natural Language Processing with Python: Analyzing Text with the Natural Language Toolkit. O'Reilly Media, Inc.\n",
    "\n",
    "Spolsky, J. (2003, October 8). The Absolute Minimum Every Software Developer Absolutely, Positively Must Know About Unicode and Character Sets (No Excuses!). Joel on Software. https://www.joelonsoftware.com/2003/10/08/the-absolute-minimum-every-software-developer-absolutely-positively-must-know-about-unicode-and-character-sets-no-excuses/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2312c92",
   "metadata": {},
   "source": [
    "# Who is discussed in the Stanford Encyclopedia?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ee71ec",
   "metadata": {},
   "source": [
    "## Version 1: Named Entity Recognition\n",
    "1. Load SEP corpus\n",
    "2. Recognize Named Entities with spaCy\n",
    "3. Display results for places, people, and organizations.\n",
    "4. split people results by Gender?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725adb01",
   "metadata": {},
   "source": [
    "## Version 2: Gazetteer\n",
    "Earlier we wrote that an NER model should not just remember a list of companies, but learn to infer from context what a company is. But if we are looking for specific enough strings, there may be other approaches. There are a lot of philosophers, but also not a LOT of philosophers. It is not a crazy thought to get them all into a list (unlike, for example, compiling a list of 'names of people in general')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673ba47b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "name": "ner-1_GAP12.ipynb",
  "toc": {
   "base_numbering": 1,
   "nav_menu": null,
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": null,
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
